{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**INSTALL AND IMPORT PACKAGES**"
      ],
      "metadata": {
        "id": "wGswBeVgdsVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePbLIWjvSSNr"
      },
      "outputs": [],
      "source": [
        "# --- Combined installation for better performance ---\n",
        "!pip install pandas numpy matplotlib seaborn geopandas fiona contextily tqdm thefuzz sentence-transformers torch statsmodels\n",
        "\n",
        "# --- Import Section (organized by functionality) ---\n",
        "\n",
        "# System & File Operations\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import glob\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "from IPython.display import display\n",
        "\n",
        "# Data Manipulation & Analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Tuple, Dict, Union\n",
        "import time\n",
        "\n",
        "# Geographic Data Processing\n",
        "import geopandas as gpd\n",
        "import fiona\n",
        "from fiona.errors import DriverError\n",
        "from shapely.ops import unary_union\n",
        "import contextily as ctx\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Text Processing & Fuzzy Matching\n",
        "from thefuzz import process\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "# Time Series Analysis\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b8sUIH0TmJ"
      },
      "source": [
        "# **EXTRACT TRANSFORM LOAD (ETL)**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyPnpkfw7For"
      },
      "source": [
        "A Methodical Approach to Dataset Analysis\n",
        "\n",
        "To ensure a robust and tailored cleaning process for each dataset, we will follow a systematic, three-step analytical workflow. This approach avoids the critical pitfall of applying a one-size-fits-all cleaning script to datasets that may have unique structures or formats. Each dataset will be treated as a separate case, requiring its own specific harmonization strategy.\n",
        "\n",
        "The workflow for each dataset is as follows:\n",
        "\n",
        "1.  **Preliminary Inspection**: We will begin with a quick, high-level overview of the raw DataFrame using `.info()` and `.head()`. The primary goal of this step is to assess the basic structure, identify the initial data types, and—most importantly—verify if the first row contains actual data headers or removable metadata. This prevents the accidental deletion of valid data.\n",
        "\n",
        "2.  **Detailed Analysis & Automated Cleaning**: Based on the preliminary inspection, we will apply the `inspect_and_clean_data` function. This function automates the initial cleaning pass, including data type conversion (dates and numerics) and generates a comprehensive data quality report. The report includes dimensions, missing values, descriptive statistics, and an analysis of categorical variables.\n",
        "\n",
        "3.  **Critical Review & Harmonization Strategy**: The output from the function **is not the final step but a diagnostic tool**. We will critically review the report to identify remaining inconsistencies, such as non-standard geographical names or codes. Based on this review, we will define and execute a specific, targeted harmonization and cleaning strategy for that dataset before it can be considered ready for merging or further analysis.\n",
        "Null values and outliers treatment, univariate and multivariate analysis will be perform in Data Cleaning and EDA Section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Datatset Loading Fucntioncs\n",
        "\n",
        "This function automatically load the differents datatabases and geodatabases required for the final mapping"
      ],
      "metadata": {
        "id": "ITNtOH1mB6ch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3970f001"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==============================================================================\n",
        "# DYNAMIC DATASET LOADER\n",
        "# ------------------------------------------------------------------------------\n",
        "# This script dynamically discovers, cleans, and loads multiple dataset files\n",
        "# (CSVs, Shapefiles, and all layers from Geodatabases) from a specified\n",
        "# directory and its subdirectories. Its sole purpose is to load all available\n",
        "# data without any filtering, making it available for downstream analysis.\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "def load_datasets(root_path: str, useless_words: list) -> dict:\n",
        "    \"\"\"\n",
        "    Discovers, cleans, and loads all available dataset files from a directory.\n",
        "\n",
        "    This function searches for .csv, .shp files, and .gdb directories. For each\n",
        "    Geodatabase, it iterates through and loads ALL available layers without\n",
        "    any selection. It generates a clean, unique programmatic key for each\n",
        "    dataset/layer and loads them into a dictionary for later use.\n",
        "\n",
        "    Args:\n",
        "        root_path (str): The root directory to search for datasets.\n",
        "        useless_words (list): A list of common words to remove from filenames\n",
        "                              to generate cleaner keys.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are cleaned dataset names and values are\n",
        "              the loaded pandas or geopandas objects.\n",
        "    \"\"\"\n",
        "    print(\"Searching for datasets...\")\n",
        "    # Discover all potential data files recursively\n",
        "    csv_files = glob.glob(os.path.join(root_path, '**', '*.csv'), recursive=True)\n",
        "    shp_files = glob.glob(os.path.join(root_path, '**', '*.shp'), recursive=True)\n",
        "    gdb_dirs = glob.glob(os.path.join(root_path, '**', '*.gdb'), recursive=True)\n",
        "\n",
        "    file_list = csv_files + shp_files + gdb_dirs\n",
        "    print(f\"Found {len(file_list)} potential datasets (.csv, .shp, .gdb). Starting bulk loading...\\n\")\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    # Pre-compile a regex pattern for cleaning words for efficiency\n",
        "    useless_pattern = r'\\b(' + '|'.join(re.escape(word) for word in useless_words) + r')\\b'\n",
        "\n",
        "    def clean_and_create_key(name_string: str) -> str:\n",
        "        \"\"\"Helper function to clean a string and create a programmatic key.\"\"\"\n",
        "        clean_name = name_string.lower().replace('-', ' ').replace('_', ' ')\n",
        "        clean_name = re.sub(useless_pattern, '', clean_name, flags=re.IGNORECASE)\n",
        "        clean_name = re.sub(r'\\s+', ' ', clean_name).strip()\n",
        "        return clean_name.replace(' ', '_')\n",
        "\n",
        "    for full_path in file_list:\n",
        "        try:\n",
        "            # --- Handle Shapefiles and CSVs (single data source files) ---\n",
        "            if full_path.endswith('.shp') or full_path.endswith('.csv'):\n",
        "                base_name = os.path.splitext(os.path.basename(full_path))[0]\n",
        "                key_name = clean_and_create_key(base_name)\n",
        "\n",
        "                if not key_name:\n",
        "                    print(f\"File: '{os.path.basename(full_path)}'\\n -> Skipped: No valid key name after cleaning. 🤷\\n\")\n",
        "                    continue\n",
        "\n",
        "                if full_path.endswith('.shp'):\n",
        "                    datasets[key_name] = gpd.read_file(full_path)\n",
        "                    print(f\"Shapefile: '{os.path.basename(full_path)}'\\n -> Loaded as key: '{key_name}' (GeoDataFrame) ✅\\n\")\n",
        "                else:\n",
        "                    datasets[key_name] = pd.read_csv(full_path)\n",
        "                    print(f\"CSV: '{os.path.basename(full_path)}'\\n -> Loaded as key: '{key_name}' (DataFrame) ✅\\n\")\n",
        "\n",
        "            # --- Handle Geodatabases (multi-layer containers) ---\n",
        "            elif full_path.endswith('.gdb'):\n",
        "                gdb_base_name = os.path.splitext(os.path.basename(full_path))[0]\n",
        "                gdb_base_key = clean_and_create_key(gdb_base_name)\n",
        "\n",
        "                print(f\"Geodatabase: '{os.path.basename(full_path)}'. Inspecting for layers to load...\")\n",
        "\n",
        "                # This step gets a list of ALL layers. No filtering occurs.\n",
        "                all_layers = fiona.listlayers(full_path)\n",
        "                if not all_layers:\n",
        "                    print(\" -> No layers found. Skipping. 🤷\\n\")\n",
        "                    continue\n",
        "\n",
        "                # This loop iterates through EVERY layer found. No selection occurs.\n",
        "                for layer_name in all_layers:\n",
        "                    layer_key_part = clean_and_create_key(layer_name)\n",
        "                    # A unique key is created to store the distinct layer in the dictionary\n",
        "                    final_key = f\"{gdb_base_key}_{layer_key_part}\" if gdb_base_key else layer_key_part\n",
        "\n",
        "                    if not final_key:\n",
        "                        print(f\"  -> Layer '{layer_name}' skipped: No valid key name after cleaning. 🤷\")\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        # The specific layer is explicitly loaded.\n",
        "                        datasets[final_key] = gpd.read_file(full_path, layer=layer_name)\n",
        "                        print(f\"  -> Layer: '{layer_name}'\\n     -> Loaded as key: '{final_key}' (GeoDataFrame) ✅\\n\")\n",
        "                    except Exception as layer_error:\n",
        "                        print(f\"  -> Failed to load layer '{layer_name}': {layer_error} ❌\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Critical error processing '{os.path.basename(full_path)}': {e} ❌\\n\")\n",
        "\n",
        "    return datasets# The function returns a clean, manageable dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Selected Database\"\n",
        "\n",
        "useless_words = ['hdx', 'hapi', 'hti', 'wfp', 'data', 'for', 'long', 'full', 'adm2', 'haiti', 'in']\n",
        "\n",
        "loaded_data = load_datasets(file_path, useless_words)"
      ],
      "metadata": {
        "id": "po1Lha65EW0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzmry3m9dtrl"
      },
      "source": [
        "## Defining  Dataset Inspection and Georeferencing Funcintions\n",
        "\n",
        "Those functions are designed to inspect and georeference different datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_geodatabase(datasets: Dict[str, Union[gpd.GeoDataFrame, pd.DataFrame]]):\n",
        "    \"\"\"\n",
        "    Inspects a dictionary of pre-loaded datasets, automatically identifies\n",
        "    which ones are GeoDataFrames, and provides a basic analysis for them.\n",
        "\n",
        "    Args:\n",
        "        datasets (Dict[str, Union[gpd.GeoDataFrame, pd.DataFrame]]):\n",
        "            A dictionary where keys are dataset names and values are the loaded\n",
        "            DataFrame or GeoDataFrame objects.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Validate the input ---\n",
        "    # Ensure the input is a non-empty dictionary.\n",
        "    if not isinstance(datasets, dict) or not datasets:\n",
        "        print(\"Error: Please provide a non-empty dictionary of loaded datasets.\")\n",
        "        return\n",
        "\n",
        "    print(f\"--- Inspecting {len(datasets)} loaded dataset(s) for geospatial data ---\")\n",
        "\n",
        "    # --- Step 2: Identify and count GeoDataFrames first ---\n",
        "    # This provides a clear summary before diving into the details.\n",
        "    geospatial_datasets = {name: data for name, data in datasets.items() if isinstance(data, gpd.GeoDataFrame)}\n",
        "\n",
        "    if not geospatial_datasets:\n",
        "        print(\"\\nNo geospatial datasets (GeoDataFrames) found to analyze.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(geospatial_datasets)} geospatial dataset(s) to analyze.\")\n",
        "\n",
        "    # --- Step 3: Iterate ONLY through the identified geospatial datasets ---\n",
        "    for name, gdf in geospatial_datasets.items():\n",
        "        print(f\"\\n--- Analyzing Geospatial Dataset: '{name}' ---\")\n",
        "        try:\n",
        "            # A) Print basic information\n",
        "            print(\"\\nGeoDataFrame Info:\")\n",
        "            gdf.info(memory_usage='deep')\n",
        "\n",
        "            # B) Show the first few rows of the attribute table\n",
        "            print(\"\\nAttribute Table Head:\")\n",
        "            print(gdf.head())\n",
        "\n",
        "            # C) Display the Coordinate Reference System (CRS)\n",
        "            print(f\"\\nCoordinate Reference System (CRS): {gdf.crs}\")\n",
        "\n",
        "            # D) Generate a basic plot for visual inspection\n",
        "            print(\"Generating plot...\")\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "            gdf.plot(ax=ax)\n",
        "            ax.set_title(f\"Map of: {name}\")\n",
        "            ax.set_xlabel(\"Longitude\")\n",
        "            ax.set_ylabel(\"Latitude\")\n",
        "            ax.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while analyzing dataset '{name}'. Error: {e}\")"
      ],
      "metadata": {
        "id": "7prGMFPajiYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EkAcT0IWdcJ"
      },
      "outputs": [],
      "source": [
        "def inspect_and_clean_data(\n",
        "    df: pd.DataFrame,\n",
        "    df_name: str,\n",
        "    remove_hdx_header: bool = True,\n",
        "    date_columns: list = None,\n",
        "    numeric_conversion_threshold: float = 0.05\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Processes, cleans, and reports on a DataFrame, with flexible controls.\n",
        "\n",
        "    This function can optionally remove a standard HDX metadata header, convert\n",
        "    specified or auto-detected columns to datetime, and attempt to convert\n",
        "    mostly-numeric columns to a numeric type.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Input DataFrame to be cleaned.\n",
        "        df_name (str): Name of the dataset for reporting purposes.\n",
        "        remove_hdx_header (bool): If True, removes the first row (index 0).\n",
        "        date_columns (list, optional): A list of column names to convert to datetime.\n",
        "                                       If None, auto-detects based on keywords.\n",
        "        numeric_conversion_threshold (float): A value between 0 and 1. Columns where the\n",
        "                                              percentage of non-numeric values is BELOW this\n",
        "                                              threshold will be force-converted to numeric.\n",
        "                                              Set to 0 to disable.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The cleaned and processed DataFrame.\n",
        "    \"\"\"\n",
        "    df_cleaned = df.copy() # Work on a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    # --- 1. Initial Data Preparation (Conditional) ---\n",
        "    if remove_hdx_header:\n",
        "        # Optional: add a check to see if the first row looks like a header\n",
        "        if isinstance(df_cleaned.iloc[0, 0], str) and '#' in df_cleaned.iloc[0, 0]:\n",
        "             df_cleaned = df_cleaned.drop(df.index[0]).reset_index(drop=True)\n",
        "        else:\n",
        "            print(f\"WARNING: Header row of '{df_name}' was not dropped because it doesn't seem to be a standard HDX header.\")\n",
        "\n",
        "    # --- 2. Data Type Conversion ---\n",
        "    # Date conversion with explicit override\n",
        "    if date_columns is None:\n",
        "        date_keywords = ['date', 'period', 'survey', 'time', 'year', 'reference', 'start']\n",
        "        date_columns = [\n",
        "            col for col in df_cleaned.columns\n",
        "            if any(keyword.lower() in col.lower() for keyword in date_keywords)\n",
        "        ]\n",
        "        print(f\"Auto-detected date columns: {date_columns}\")\n",
        "\n",
        "    for col_name in date_columns:\n",
        "        if col_name in df_cleaned.columns:\n",
        "            df_cleaned[col_name] = pd.to_datetime(df_cleaned[col_name], errors='coerce')\n",
        "\n",
        "    # Numeric conversion with explicit threshold and warning\n",
        "    potential_numeric_cols = [col for col in df_cleaned.columns if col not in date_columns]\n",
        "    for col in potential_numeric_cols:\n",
        "        # Try conversion only on object columns to be efficient\n",
        "        if df_cleaned[col].dtype == 'object':\n",
        "            numeric_values = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
        "            original_nulls = df_cleaned[col].isnull().sum()\n",
        "            new_nulls = numeric_values.isnull().sum()\n",
        "\n",
        "            # Check if the number of new NaNs is within the threshold\n",
        "            if 0 <= (new_nulls - original_nulls) < (len(df_cleaned) * numeric_conversion_threshold):\n",
        "                print(f\"WARNING: Column '{col}' in '{df_name}' was force-converted to numeric, creating {new_nulls - original_nulls} new NaN values.\")\n",
        "                df_cleaned[col] = numeric_values\n",
        "\n",
        "    # --- 3. Data Quality Reporting (invariato) ---\n",
        "    print(\"\\n\" + \"=\" * 75)\n",
        "    print(f\"--- Data Quality Report for {df_name} ---\")\n",
        "    print(\"=\" * 75)\n",
        "\n",
        "    # Basic DataFrame information\n",
        "\n",
        "    print(\"\\nDataFrame Overview:\")\n",
        "\n",
        "    df_cleaned.info()\n",
        "\n",
        "   # Data samples\n",
        "\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "\n",
        "    display(df_cleaned.head(5))\n",
        "\n",
        "    print(\"\\nLast 5 rows:\")\n",
        "\n",
        "    display(df_cleaned.tail(5))\n",
        "\n",
        "    # Structural information\n",
        "\n",
        "    print(\"\\nData Dimensions:\", df_cleaned.shape)\n",
        "\n",
        "    print(\"\\nMissing Values per Column:\")\n",
        "\n",
        "    print(df_cleaned.isnull().sum())\n",
        "\n",
        "    print(\"\\nDescriptive Statistics:\")\n",
        "\n",
        "    print(df_cleaned.describe())\n",
        "\n",
        "   # Categorical analysis\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 75)\n",
        "\n",
        "    print(\"--- Categorical Variables Analysis ---\")\n",
        "\n",
        "    print(\"=\" * 75)\n",
        "\n",
        "    categorical_cols = df_cleaned.select_dtypes(include=['object', 'datetime64']).columns\n",
        "\n",
        "    for col in categorical_cols:\n",
        "      n_unique = df_cleaned[col].nunique()\n",
        "      print(\"-\" * 50)\n",
        "      print(f\"Column: '{col}' (Unique values: {n_unique})\")\n",
        "      if n_unique < 20:\n",
        "        print(\"\\nValue Distribution:\")\n",
        "        print(df_cleaned[col].value_counts())\n",
        "      elif n_unique < 50:\n",
        "        print(\"\\nUnique Values:\")\n",
        "        try:\n",
        "          print(sorted(df_cleaned[col].dropna().unique().tolist()))\n",
        "        except:\n",
        "          print(df_cleaned[col].dropna().unique())\n",
        "      else:\n",
        "        print(f\"\\nLarge number of unique values ({n_unique}) - sample output suppressed\")\n",
        "\n",
        "   # Print the name of the new cleaned dataset\n",
        "    cleaned_name = f\"{df_name}_cleaned\"\n",
        "    print(\"\\n\" + \"=\" * 75)\n",
        "    print(f\"Created cleaned dataset: {cleaned_name}\")\n",
        "    print(f\"--- {df_name} Processing Complete ---\")\n",
        "    print(\"=\" * 75)\n",
        "\n",
        "    return df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op_n9Fkc8rgE"
      },
      "outputs": [],
      "source": [
        "def create_canonical_geotable(\n",
        "    datasets: dict,\n",
        "    adm1_key: str,\n",
        "    adm0_key: str = None,\n",
        "    adm2_key: str = None,\n",
        "    adm3_key: str = None\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Creates a flexible, canonical, and enriched geographic dataset by hierarchically\n",
        "    merging up to four levels of administrative boundaries.\n",
        "\n",
        "    The function uses the most granular level provided (e.g., ADM3) as the base\n",
        "    and progressively merges information from higher administrative levels. ADM1 is\n",
        "    a mandatory base level.\n",
        "\n",
        "    Args:\n",
        "        datasets (dict): Dictionary containing pre-loaded GeoDataFrames.\n",
        "        adm1_key (str): The dictionary key for the mandatory ADM1 (Departments) GeoDataFrame.\n",
        "        adm0_key (str, optional): Key for the ADM0 (National) GeoDataFrame. Defaults to None.\n",
        "        adm2_key (str, optional): Key for the ADM2 (Communes) GeoDataFrame. Defaults to None.\n",
        "        adm3_key (str, optional): Key for the ADM3 (Communal Sections) GeoDataFrame. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: An enriched GeoDataFrame with combined administrative data,\n",
        "                          or an empty GeoDataFrame on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"--- Creating dynamic canonical geographic table ---\")\n",
        "\n",
        "        # --- 1. Standardize and process each provided administrative level ---\n",
        "\n",
        "        level_keys = {\n",
        "            'adm0': adm0_key, 'adm1': adm1_key,\n",
        "            'adm2': adm2_key, 'adm3': adm3_key\n",
        "        }\n",
        "        processed_data = {}\n",
        "\n",
        "        # Corrected column mapping for standardization\n",
        "        COLUMN_MAP = {\n",
        "            'adm0': {'Pcode0': 'admin0Pcode', 'adm0_name_en': 'admin0Name_en', 'adm0_name_fr': 'admin0Name_fr'},\n",
        "            'adm1': {'Pcode1': 'admin1Pcode', 'adm1_name_en': 'admin1Name_en', 'adm1_name_fr': 'admin1Name_fr', 'Pcode0': 'admin0Pcode'},\n",
        "            'adm2': {'Pcode2': 'admin2Pcode', 'adm2_name_en': 'admin2Name_en', 'adm2_name_fr': 'admin2Name_fr', 'Pcode1': 'admin1Pcode'},\n",
        "            'adm3': {'Pcode3': 'admin3Pcode', 'adm3_name_en': 'admin3Name_en', 'adm3_name_fr': 'admin3Name_fr', 'Pcode2': 'admin2Pcode'}\n",
        "        }\n",
        "\n",
        "        for level, key in level_keys.items():\n",
        "            if key:\n",
        "                print(f\"Processing {level.upper()}...\")\n",
        "                gdf = datasets[key]\n",
        "                level_map = COLUMN_MAP[level]\n",
        "\n",
        "                # Invert map for renaming: {'original_name': 'new_name'}\n",
        "                rename_map = {v: k for k, v in level_map.items() if v in gdf.columns}\n",
        "                cols_to_select = list(rename_map.keys())\n",
        "\n",
        "                # Always include geometry\n",
        "                if 'geometry' not in cols_to_select:\n",
        "                    cols_to_select.append('geometry')\n",
        "\n",
        "                processed_gdf = gdf[cols_to_select].rename(columns=rename_map)\n",
        "                processed_data[level] = processed_gdf\n",
        "\n",
        "        # --- 2. Determine the base GeoDataFrame (the most detailed level) ---\n",
        "\n",
        "        if 'adm3' in processed_data:\n",
        "            base_level = 'adm3'\n",
        "        elif 'adm2' in processed_data:\n",
        "            base_level = 'adm2'\n",
        "        else:\n",
        "            base_level = 'adm1'\n",
        "\n",
        "        print(f\"Using {base_level.upper()} as the base for the final table.\")\n",
        "        final_table = processed_data[base_level].copy()\n",
        "\n",
        "        # --- 3. Hierarchically merge parent-level data ---\n",
        "\n",
        "        # Merge ADM2 info if base is ADM3\n",
        "        if base_level == 'adm3' and 'adm2' in processed_data:\n",
        "            adm2_info = processed_data['adm2'].drop(columns='geometry')\n",
        "            final_table = pd.merge(final_table, adm2_info, on='Pcode2', how='left')\n",
        "\n",
        "        # Merge ADM1 info if base is ADM2 or ADM3\n",
        "        if base_level in ['adm3', 'adm2'] and 'adm1' in processed_data:\n",
        "            adm1_info = processed_data['adm1'].drop(columns='geometry')\n",
        "            final_table = pd.merge(final_table, adm1_info, on='Pcode1', how='left')\n",
        "\n",
        "        # Merge ADM0 info (if available and needed)\n",
        "        if base_level in ['adm3', 'adm2', 'adm1'] and 'adm0' in processed_data:\n",
        "            adm0_info = processed_data['adm0'].drop(columns='geometry')\n",
        "            final_table = pd.merge(final_table, adm0_info, on='Pcode0', how='left')\n",
        "\n",
        "        # --- 4. Calculate Centroids for the base geometry ---\n",
        "\n",
        "        print(\"Calculating centroids...\")\n",
        "        projected_crs = \"EPSG:32618\"  # Projected CRS for Haiti\n",
        "        original_crs = final_table.crs\n",
        "        final_table = final_table.set_geometry('geometry')\n",
        "        centroids = final_table.geometry.to_crs(projected_crs).centroid.to_crs(original_crs)\n",
        "        final_table['longitude'] = centroids.x\n",
        "        final_table['latitude'] = centroids.y\n",
        "\n",
        "        print(\"✅ Dynamic canonical geographic table created successfully.\")\n",
        "        return final_table\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"🚨 ERROR: A required key or column name was not found: {e}. Check input keys and source data structure.\")\n",
        "        return gpd.GeoDataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"🚨 ERROR: Could not create canonical table. Details: {e}\")\n",
        "        return gpd.GeoDataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmLZaSkJ0M_k"
      },
      "outputs": [],
      "source": [
        "def harmonize_and_georeference(\n",
        "    df_to_harmonize: pd.DataFrame,\n",
        "    georef_layers: dict,\n",
        "    georef_key: str,\n",
        "    source_col: Union[str, List[str]],\n",
        "    canonical_col: Union[str, List[str]]\n",
        ") -> gpd.GeoDataFrame:\n",
        "    \"\"\"\n",
        "    Georeferences a DataFrame by joining it with a specific layer from a\n",
        "    dictionary of pre-processed canonical GeoDataFrames.\n",
        "\n",
        "    Args:\n",
        "        df_to_harmonize (pd.DataFrame): The tabular DataFrame to be georeferenced.\n",
        "        georef_layers (dict): The dictionary containing the canonical GeoDataFrames\n",
        "                              (e.g., the outputs from create_canonical_geotable).\n",
        "        georef_key (str): The key specifying which GeoDataFrame in the dictionary\n",
        "                          to use for the join (e.g., 'gdf_adm2').\n",
        "        source_col (Union[str, List[str]]): The join key column(s) in the source DataFrame.\n",
        "        canonical_col (Union[str, List[str]]): The corresponding join key column(s)\n",
        "                                             in the canonical GeoDataFrame.\n",
        "\n",
        "    Returns:\n",
        "        gpd.GeoDataFrame: The enriched and georeferenced DataFrame, or an empty\n",
        "                          GeoDataFrame on error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Input Validation and Layer Selection ---\n",
        "\n",
        "        # Retrieve the specified canonical GeoDataFrame from the dictionary\n",
        "        if georef_key not in georef_layers:\n",
        "            print(f\"❌ Error: The georeference key '{georef_key}' was not found in the provided dictionary.\")\n",
        "            return gpd.GeoDataFrame()\n",
        "        canonical_gdf = georef_layers[georef_key]\n",
        "\n",
        "        # Ensure column inputs are lists for consistent processing\n",
        "        source_cols = [source_col] if isinstance(source_col, str) else source_col\n",
        "        canonical_cols = [canonical_col] if isinstance(canonical_col, str) else canonical_col\n",
        "\n",
        "        # Check for column existence and matching list lengths\n",
        "        if not all(col in df_to_harmonize.columns for col in source_cols):\n",
        "            missing = [col for col in source_cols if col not in df_to_harmonize.columns]\n",
        "            print(f\"❌ Error: Source column(s) not found in the DataFrame: {missing}\")\n",
        "            return gpd.GeoDataFrame()\n",
        "        if not all(col in canonical_gdf.columns for col in canonical_cols):\n",
        "            missing = [col for col in canonical_cols if col not in canonical_gdf.columns]\n",
        "            print(f\"❌ Error: Canonical column(s) not found in the GeoDataFrame: {missing}\")\n",
        "            return gpd.GeoDataFrame()\n",
        "        if len(source_cols) != len(canonical_cols):\n",
        "            print(\"❌ Error: The number of source and canonical columns must be identical for joining.\")\n",
        "            return gpd.GeoDataFrame()\n",
        "\n",
        "        # --- 2. Optimized Merge Operation ---\n",
        "\n",
        "        # Select only the necessary columns from the canonical GDF: join keys and geometry\n",
        "        # This prevents column name conflicts (e.g., 'adm1_name' in both DFs) and is more efficient.\n",
        "        cols_to_keep = canonical_cols + ['geometry']\n",
        "        geodata_subset = canonical_gdf[cols_to_keep]\n",
        "\n",
        "        # Perform the left join\n",
        "        result_df = pd.merge(\n",
        "            df_to_harmonize,\n",
        "            geodata_subset,\n",
        "            left_on=source_cols,\n",
        "            right_on=canonical_cols,\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # --- 3. Verification and Reporting ---\n",
        "\n",
        "        # Check for unmatched rows by looking for nulls in the merged geometry column\n",
        "        unmatched_mask = result_df['geometry'].isnull()\n",
        "        if unmatched_mask.any():\n",
        "            unmatched_count = unmatched_mask.sum()\n",
        "            # Get a sample of the actual values from the source DF that failed to match\n",
        "            unmatched_examples = df_to_harmonize[unmatched_mask][source_cols].drop_duplicates().head(5)\n",
        "            print(f\"⚠️ {unmatched_count} out of {len(df_to_harmonize)} rows could not be georeferenced.\")\n",
        "            print(\"Example unmatched source values:\")\n",
        "            print(unmatched_examples.to_string())\n",
        "        else:\n",
        "            print(\"✅ All rows were successfully georeferenced.\")\n",
        "\n",
        "        # If all rows failed to match, return an empty GDF\n",
        "        if unmatched_mask.all():\n",
        "            print(\"❌ Critical: No rows could be matched. Aborting.\")\n",
        "            return gpd.GeoDataFrame()\n",
        "\n",
        "        # --- 4. Finalize GeoDataFrame ---\n",
        "\n",
        "        # Drop rows that were not matched\n",
        "        georeferenced_gdf = result_df[~unmatched_mask].copy()\n",
        "\n",
        "        # If the original join columns are different, drop the ones from the canonical set\n",
        "        if source_cols != canonical_cols:\n",
        "            georeferenced_gdf = georeferenced_gdf.drop(columns=canonical_cols)\n",
        "\n",
        "        # Convert the final DataFrame to a GeoDataFrame\n",
        "        return gpd.GeoDataFrame(\n",
        "            georeferenced_gdf,\n",
        "            geometry='geometry',\n",
        "            crs=canonical_gdf.crs\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"🚨 An unexpected error occurred during harmonization: {e}\")\n",
        "        return gpd.GeoDataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2hKvzdMd3vn"
      },
      "source": [
        "## Loading Geodatabase for Administrative Zones and Livelihood Zones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#--- Explore Geodatabase ---\n",
        "# First visualization of:\n",
        "# 1. hotosm_roads_lines_shp : Shapefile for haitian road from Humantiarian Strett Map\n",
        "# 2. ht_lhz_2015 : Shapefile for Livelihood Zones in Haiti as defined by Fewsnet\n",
        "# 3. 6 GeoDatabase of Nation and Subnational Administrative Boundaries from OCHA Haiti\n",
        "explore_geodatabase(loaded_data)"
      ],
      "metadata": {
        "id": "cXT9WnHzDYh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVH_RsRrTDyo"
      },
      "outputs": [],
      "source": [
        "# --- 1. Load specific layers from Haiti Geodatabase ---\n",
        "print(\"--- Loading data from Haiti Geodatabase ---\")\n",
        "\n",
        "# Selecting the specific layers for the analysis\n",
        "key_adm0 = 'adminboundaries_candidate_admbnda_adm0_cnigs_20181129' # National Haitian Map\n",
        "key_adm1 = 'adminboundaries_candidate_admbnda_adm1_cnigs_20181129' # Subnational - Department\n",
        "key_adm2 = 'adminboundaries_candidate_admbnda_cnigs_20181129'      # Subnational - Municipilaties\n",
        "key_adm3 ='adminboundaries_candidate_admbnda_adm3_cnigs_20181129'  # Subnation - Municipilaties Section\n",
        "\n",
        "#create national level gdf\n",
        "gdf_admin0 = create_canonical_geotable(datasets=loaded_data,\n",
        "                                                     adm0_key=key_adm0,\n",
        "                                                     adm1_key=key_adm1\n",
        ")\n",
        "print(\"/n\", print(gdf_admin0.head()))\n",
        "\n",
        "#create department level gdf\n",
        "gdf_admin1= create_canonical_geotable(datasets=loaded_data,\n",
        "                                                     adm0_key=key_adm0,\n",
        "                                                     adm1_key=key_adm1\n",
        ")\n",
        "print(gdf_admin1.head())\n",
        "\n",
        "#create commune level gdf\n",
        "\n",
        "gdf_admin2= create_canonical_geotable(datasets=loaded_data,\n",
        "                                                     adm0_key=key_adm0,\n",
        "                                                     adm1_key=key_adm1,\n",
        "                                                     adm2_key=key_adm2\n",
        ")\n",
        "print(gdf_admin2.head())\n",
        "\n",
        "#create commune section level gdf\n",
        "gdf_admin3= create_canonical_geotable(datasets=loaded_data,\n",
        "                                                     adm0_key=key_adm0,\n",
        "                                                     adm1_key=key_adm1,\n",
        "                                                     adm2_key=key_adm2,\n",
        "                                                     adm3_key=key_adm3\n",
        ")\n",
        "print(gdf_admin3.head())\n",
        "\n",
        "# Renaming Haitina Livelihood Dataset\n",
        "gdf_livelihood= loaded_data['ht_lhz_2015']\n",
        "print(\"/n\", gdf_livelihood.head())\n",
        "#Renaming Haitina Road Dataset\n",
        "ht_road= loaded_data['hotosm_roads_lines_shp']\n",
        "print(\"/n\", ht_road.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1tx2HZ11xIL"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Food Price\n",
        "\n",
        "This dataset contains Food Prices data for Haiti, sourced from the World Food Programme Price Database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRiroJFx7cfK"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# --- RAW DATA PRELIMINARY INSPECTION for `df_food_price`---\n",
        "#\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `df_food_price`: Structure and Memory Footprint ---\")\n",
        "loaded_data['food_price'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `df_food_price`: Initial Rows Sample ---\")\n",
        "display(loaded_data['food_price'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `df_food_price`: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['food_price'].shape[0]} rows and {loaded_data['food_price'].shape[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains an header that modify alla raw type in object\n",
        "\n",
        "Two columns (reference_period_start and reference_period_end) identified as objects are time columns.\n",
        "\n",
        "No null values"
      ],
      "metadata": {
        "id": "6p6goMtXaqA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2rNCki7l5Vm"
      },
      "outputs": [],
      "source": [
        "#--- CONFIGURED EXECUTION of inspect_and_clean_data for food price ---\n",
        "#\n",
        "# Default numeric conversion threshold will be used as it is a reasonable\n",
        "#  starting point for this dataset.\n",
        "\n",
        "# Inspect and renaming the database for clarity.\n",
        "df_food_price_cleaned = inspect_and_clean_data(\n",
        "    df=loaded_data['food_price']\n",
        "    ,df_name=\"food_price\",\n",
        "    remove_hdx_header=True,\n",
        "    date_columns=['reference_period_start', 'reference_period_end'])\n",
        "\n",
        "print(\"\\n--- [RAW] `df_food_price`: Initial Rows Sample ---\")\n",
        "display(loaded_data['food_price'].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inspect and cleaning process was succesfull. All columns were converted correctly creating 0 null values.\n",
        "\n",
        "In this section no"
      ],
      "metadata": {
        "id": "1cskA7qvhLq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCdnJoIHnNnx"
      },
      "outputs": [],
      "source": [
        "#--- GEOSPATIAL HARMONIZATION USING A CANONICAL SOURCE (GEODATABASE) ---\n",
        "\n",
        "# Objective: Clean, standardize, and enrich location data using semantic matching.\n",
        "\n",
        "# ---  1. Call the harmonization function ---\n",
        "#creating georef_layers variables for harmonizing. this variables will be used also for other dataset\n",
        "georef_layers = {\n",
        "    'gdf_adm1' : gdf_admin1,\n",
        "    'gdf_adm2': gdf_admin2,\n",
        "    'gdf_adm3': gdf_admin3 # optinal. for in detail analysis\n",
        "}\n",
        "# Now the 'georef_layers' variable exists and can be used.\n",
        "df_food_price_final = harmonize_and_georeference(\n",
        "    df_to_harmonize=df_food_price_cleaned,\n",
        "    georef_layers=georef_layers,\n",
        "    georef_key='gdf_adm2', # Specify which layer to use for the join\n",
        "    source_col='admin2_name', # english laguange selected - frenahc and creole also available\n",
        "    canonical_col='adm2_name_en'\n",
        ")\n",
        "\n",
        "# --- 2. Display the result ---\n",
        "print(\"\\\\n--- Preview of the final georeferenced food price data ---\")\n",
        "display(df_food_price_final.head())\n",
        "print(f\"CRS of the final GeoDataFrame: {df_food_price_final.crs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arOVQU7U3E4n"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Food Security\n",
        "\n",
        "The IPC Acute Food Insecurity (IPC AFI) classification provides strategically relevant information to decision makers that focuses on short-term objectives to prevent, mitigate or decrease severe food insecurity that threatens lives or livelihoods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqK7lWSr9IxJ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: RAW DATA PRELIMINARY INSPECTION for `df_food_security`\n",
        "# ==============================================================================\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `df_food_security`: Structure and Memory Footprint ---\")\n",
        "loaded_data['food_security'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `df_food_scurity`: Initial Rows Sample ---\")\n",
        "display(loaded_data['food_security'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `df_food_security`: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['food_security'].shape[0]} rows and {loaded_data['food_security'].shape[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This datataset containts two columns with date type (reference_period_start and reference_period_end)\n",
        "\n",
        "Some Columns (admin1_name and admin2_code) have only 1 values.\n",
        "\n",
        "There are also several missing values in 4 columns (provider_admin1_name , provider_admin2_name , admin1_code, admin1_name)\n",
        "\n",
        "This database required a substantial intervention to adress those issues.\n",
        "\n",
        "It laso has a first row header.                    "
      ],
      "metadata": {
        "id": "9spwgcbvi5Tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvZRrliRxdMV"
      },
      "outputs": [],
      "source": [
        "#==============================================================================\n",
        "#CONFIGURED EXECUTION of inspect_and_clean_data\n",
        "# ==============================================================================\n",
        "\n",
        "#    starting point for this dataset.\n",
        "\n",
        "# Now, we call the function with explicit, informed parameters.\n",
        "df_food_security_cleaned = inspect_and_clean_data(\n",
        "    df=loaded_data['food_security'],\n",
        "    df_name=\"food_security\",\n",
        "    remove_hdx_header=True,\n",
        "    date_columns=['reference_period_start', 'reference_period_end'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "admin2_code and admin2_name now are null becuase the only value was the header\n",
        "\n",
        "provider_admin1_name presents spelling error (e.g: 'Cite Soleil, Cité_Soleil)\n",
        "\n",
        "provider_admin2_name presents 93 unique values. Those values are to many to be displayed, but we can supsect the presence of spelling error like in the previous column\n",
        "\n",
        "the dataset present a mix of administriative zones and livelihood zones\n"
      ],
      "metadata": {
        "id": "mSEuvMv1lTkh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_gr2zRjX3hI"
      },
      "source": [
        "### Matching livelihood zones with administrative bounadiaries\n",
        "The food security database contains  a mix of geografic buondaries by department and livelihood zones as determined by FEWS.\n",
        "\n",
        "It addition there are several missing values\n",
        "\n",
        "In this section trough fuzzy matching and SentenceTransformer will we adress those issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQyxL6QpgRjh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =============================================================================\n",
        "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "# ==============================================================================\n",
        "# STEP 1: Build a Unified Knowledge Base for Semantic Search\n",
        "# ==============================================================================\n",
        "# To resolve ambiguity, we create a single, searchable database of all\n",
        "# canonical location and zone names.\n",
        "\n",
        "# 1a. Prepare Administrative Names (ADM1 & ADM2)\n",
        "kb_adm1 = gdf_admin2[['adm1_name_en']].drop_duplicates().rename(columns={'adm1_name_en': 'name'})\n",
        "kb_adm1['type'] = 'Department'\n",
        "kb_adm2 = gdf_admin2[['adm2_name_en']].drop_duplicates().rename(columns={'adm2_name_en': 'name'})\n",
        "kb_adm2['type'] = 'Commune'\n",
        "\n",
        "# 1b. Prepare Livelihood Zone Names (English and French for better matching)\n",
        "kb_lz_en = gdf_livelihood[['LZNAMEEN']].drop_duplicates().rename(columns={'LZNAMEEN': 'name'})\n",
        "kb_lz_en['type'] = 'Livelihood Zone'\n",
        "kb_lz_fr = gdf_livelihood[['LZNAMEFR']].drop_duplicates().rename(columns={'LZNAMEFR': 'name'})\n",
        "kb_lz_fr['type'] = 'Livelihood Zone'\n",
        "\n",
        "# 1c. Combine into a single knowledge base\n",
        "knowledge_base = pd.concat([kb_adm1, kb_adm2, kb_lz_en, kb_lz_fr], ignore_index=True).dropna().drop_duplicates(subset=['name'])\n",
        "print(f\"Knowledge base created with {len(knowledge_base)} unique names.\")\n",
        "\n",
        "# 1d. Pre-compute embeddings for all canonical names for high performance\n",
        "print(\"Pre-computing embeddings for the knowledge base...\")\n",
        "knowledge_base_embeddings = model.encode(knowledge_base['name'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# 1e. Create helper maps for fast lookups\n",
        "adm2_to_adm1_map = gdf_admin2.drop_duplicates('adm2_name_en').set_index('adm2_name_en')['adm1_name_en'].to_dict()\n",
        "lz_name_to_code_map = pd.concat([\n",
        "    gdf_livelihood.set_index('LZNAMEEN')['LZCODE'],\n",
        "    gdf_livelihood.set_index('LZNAMEFR')['LZCODE']\n",
        "]).to_dict()\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 2: Define the Main Harmonization Function\n",
        "# ==============================================================================\n",
        "\n",
        "def harmonize_food_security_locations(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire complex harmonization pipeline for the food security dataset.\n",
        "    \"\"\"\n",
        "    df_result = df.copy()\n",
        "\n",
        "    # --- 2a. Inner function to process each row with complex logic ---\n",
        "    def _process_row(row):\n",
        "        # Initialize output fields\n",
        "        adm1_final, adm2_final, liv_name = None, None, None\n",
        "        lz_codes_extracted = []\n",
        "        metadata_tag = None\n",
        "\n",
        "        # Combine provider columns into a single string for parsing\n",
        "        query_parts = []\n",
        "        admin1 = row.get('provider_admin1_name')\n",
        "        admin2 = row.get('provider_admin2_name')\n",
        "\n",
        "        if pd.notna(admin1) and str(admin1).strip():\n",
        "          query_parts.append(str(admin1))\n",
        "\n",
        "        if pd.notna(admin2) and str(admin2).strip():\n",
        "          query_parts.append(str(admin2))\n",
        "\n",
        "        search_query = \" \".join(query_parts)\n",
        "        if not search_query:\n",
        "          return None, None, 'National', None, None, None, None\n",
        "\n",
        "        # --- Parsing Stage: Extract codes, tags, and text ---\n",
        "        # Extract all HT codes (e.g., HT01, ht07)\n",
        "        lz_codes_extracted = re.findall(r'HT\\d+', search_query, re.IGNORECASE)\n",
        "        lz_codes_extracted = [code.upper() for code in lz_codes_extracted]\n",
        "\n",
        "        # Extract P+TP tag\n",
        "        if 'P+TP' in search_query:\n",
        "            metadata_tag = 'P+TP'\n",
        "\n",
        "        # Isolate the clean text for semantic matching\n",
        "        text_query = re.sub(r'HT\\d+', '', search_query, flags=re.IGNORECASE)\n",
        "        text_query = text_query.replace('P+TP', '').strip()\n",
        "\n",
        "        # --- Reconciliation Stage ---\n",
        "        harmonized_name = None\n",
        "\n",
        "        if text_query:\n",
        "            # Find the best semantic match for the text part\n",
        "            query_embedding = model.encode(text_query, convert_to_tensor=True)\n",
        "            cos_scores = util.pytorch_cos_sim(query_embedding, knowledge_base_embeddings)[0]\n",
        "            best_match_idx = torch.argmax(cos_scores).item()\n",
        "            best_match_score = cos_scores[best_match_idx].item()\n",
        "\n",
        "            if best_match_score > 0.7: # Confidence threshold\n",
        "                best_match = knowledge_base.iloc[best_match_idx]\n",
        "                harmonized_name = best_match['name']\n",
        "\n",
        "                # Prioritize administrative boundaries for filling adm columns\n",
        "                if best_match['type'] == 'Commune':\n",
        "                    adm2_final = harmonized_name\n",
        "                    adm1_final = adm2_to_adm1_map.get(adm2_final)\n",
        "                elif best_match['type'] == 'Department':\n",
        "                    adm1_final = harmonized_name\n",
        "\n",
        "        # --- Final Assembly Stage ---\n",
        "        # 1. Build 'liv_name'\n",
        "        liv_name_parts = [harmonized_name or text_query] # Use harmonized name if available\n",
        "        if lz_codes_extracted:\n",
        "            liv_name_parts.append(\" \".join(lz_codes_extracted))\n",
        "        if metadata_tag:\n",
        "            liv_name_parts.append(metadata_tag)\n",
        "        liv_name = \" \".join(filter(None, liv_name_parts))\n",
        "\n",
        "        # 2. Populate LZ codes with replication logic\n",
        "        lz1, lz2, lz3 = None, None, None\n",
        "        if len(lz_codes_extracted) >= 3:\n",
        "            lz1, lz2, lz3 = lz_codes_extracted[0], lz_codes_extracted[1], lz_codes_extracted[2]\n",
        "        elif len(lz_codes_extracted) == 2:\n",
        "            lz1, lz2, lz3 = lz_codes_extracted[0], lz_codes_extracted[1], lz_codes_extracted[1] # Replicate\n",
        "        elif len(lz_codes_extracted) == 1:\n",
        "            lz1, lz2, lz3 = lz_codes_extracted[0], lz_codes_extracted[0], lz_codes_extracted[0] # Replicate\n",
        "\n",
        "        # 3. Handle the special case where P+TP requires spatial join\n",
        "        needs_spatial_join = (metadata_tag and not lz_codes_extracted)\n",
        "\n",
        "        return adm1_final, adm2_final, liv_name, lz1, lz2, lz3, needs_spatial_join\n",
        "\n",
        "    # --- 2b. Apply the processing function to all rows ---\n",
        "    print(\"Processing each row to extract and harmonize information...\")\n",
        "    processed_cols = ['adm1_final', 'adm2_final', 'liv_name', 'lz_code1', 'lz_code2', 'lz_code3', 'needs_spatial_join']\n",
        "    df_result[processed_cols] = df_result.apply(_process_row, axis=1, result_type='expand')\n",
        "\n",
        "    # --- 2c. Efficiently handle all spatial joins in one batch ---\n",
        "    spatial_join_subset = df_result[df_result['needs_spatial_join'] == True].copy()\n",
        "    if not spatial_join_subset.empty:\n",
        "        print(f\"Performing efficient batch spatial join for {len(spatial_join_subset)} rows...\")\n",
        "        # Create GeoDataFrame from the ADM2 geometry of the matched commune\n",
        "        spatial_join_subset = gpd.GeoDataFrame(spatial_join_subset,\n",
        "                                               geometry=spatial_join_subset['adm2_final'].map(\n",
        "                                                   gdf_admin2.set_index('adm2_name_en')['geometry']\n",
        "                                               ), crs=\"EPSG:4326\")\n",
        "\n",
        "        # Perform the spatial join\n",
        "        joined_gdf = gpd.sjoin(spatial_join_subset, gdf_livelihood, how='left', predicate='intersects')\n",
        "\n",
        "        # Update the lz_code columns based on the spatial join result\n",
        "        # Using .loc to safely update the original dataframe\n",
        "        df_result.loc[joined_gdf.index, 'lz_code1'] = joined_gdf['LZCODE']\n",
        "        # Apply replication logic\n",
        "        df_result.loc[joined_gdf.index, 'lz_code2'] = joined_gdf['LZCODE']\n",
        "        df_result.loc[joined_gdf.index, 'lz_code3'] = joined_gdf['LZCODE']\n",
        "\n",
        "    df_result.drop(columns=['needs_spatial_join'], inplace=True)\n",
        "    return df_result\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# STEP 3: EXECUTE AND REVIEW\n",
        "# ==============================================================================\n",
        "df_food_security_cleaned2 = harmonize_food_security_locations(df_food_security_cleaned)\n",
        "\n",
        "# Display a sample of the key columns to verify the results\n",
        "columns_to_review = [\n",
        "    'provider_admin1_name',\n",
        "    'provider_admin2_name',\n",
        "    'liv_name',\n",
        "    'adm1_final',\n",
        "    'adm2_final',\n",
        "    'lz_code1',\n",
        "    'lz_code2',\n",
        "    'lz_code3'\n",
        "]\n",
        "\n",
        "print(\"\\n--- Review a random sample of the final harmonized data ---\")\n",
        "display(df_food_security_cleaned2[columns_to_review].sample(500, random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UebFOKqoAuGW"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# EXPERT CHANGES ANALYSIS\n",
        "#\n",
        "# This script compares the machine-generated harmonization ('to_review')\n",
        "# with the final version corrected by the expert ('reviewed').\n",
        "# It generates summary statistics and a detailed log of all changes.\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# File paths for the two versions\n",
        "EXPERT_FILE_PATH1 = \"/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Expert Review/expert_to_review.csv\"\n",
        "EXPERT_FILE_PATH2 = \"/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Expert Review/expert_reviewed.csv\"\n",
        "\n",
        "# Columns to compare for changes\n",
        "columns_to_compare = [\n",
        "    'liv_name', 'adm1_final', 'adm2_final',\n",
        "    'lz_code1', 'lz_code2', 'lz_code3'\n",
        "]\n",
        "# Unique identifier columns for each row/rule\n",
        "key_columns = ['provider_admin1_name', 'provider_admin2_name']\n",
        "\n",
        "print(\"--- 📊 Expert Review Analysis ---\")\n",
        "\n",
        "try:\n",
        "    # --- 1. LOAD DATA ---\n",
        "    # Load the state BEFORE the expert's review\n",
        "    df_before = pd.read_csv(EXPERT_FILE_PATH1)\n",
        "    # Load the state AFTER the expert's review\n",
        "    df_after = pd.read_csv(EXPERT_FILE_PATH2)\n",
        "    print(f\"Loaded {len(df_before)} rows from 'expert_to_review.csv'\")\n",
        "    print(f\"Loaded {len(df_after)} rows from 'expert_reviewed.csv'\")\n",
        "\n",
        "    # --- 2. MERGE FOR COMPARISON ---\n",
        "    # Merge the two dataframes using the provider names as the key.\n",
        "    # This aligns each machine guess with its corresponding expert correction.\n",
        "    comparison_df = pd.merge(\n",
        "        df_before,\n",
        "        df_after,\n",
        "        on=key_columns,\n",
        "        suffixes=('_before', '_after'),\n",
        "        how='outer' # Use outer to also catch added/deleted rows\n",
        "    )\n",
        "\n",
        "    # --- 3. CALCULATE STATISTICS ---\n",
        "    total_rows = len(comparison_df)\n",
        "    changes_summary = {}\n",
        "    any_change_mask = pd.Series(False, index=comparison_df.index)\n",
        "\n",
        "    # Compare each column pair\n",
        "    for col in columns_to_compare:\n",
        "        col_before = f\"{col}_before\"\n",
        "        col_after = f\"{col}_after\"\n",
        "\n",
        "        # Treat NaN values as equal to avoid false positives\n",
        "        mask = (comparison_df[col_before] != comparison_df[col_after]) & \\\n",
        "               ~(comparison_df[col_before].isnull() & comparison_df[col_after].isnull())\n",
        "\n",
        "        changes_summary[col] = mask.sum()\n",
        "        any_change_mask = any_change_mask | mask\n",
        "\n",
        "    rows_with_changes = any_change_mask.sum()\n",
        "\n",
        "    print(\"\\n--- Summary of Changes ---\")\n",
        "    print(f\"Total Unique Rules Analyzed: {total_rows}\")\n",
        "    print(f\"Rows Modified by Expert:     {rows_with_changes} ({rows_with_changes / total_rows:.2%})\")\n",
        "    print(\"\\nChanges per Column:\")\n",
        "    for col, count in changes_summary.items():\n",
        "        print(f\"  - {col:<12}: {count} changes\")\n",
        "\n",
        "    # --- 4. CREATE DETAILED CHANGE LOG ---\n",
        "    if rows_with_changes > 0:\n",
        "        # Filter to only show rows where at least one change occurred\n",
        "        change_log_df = comparison_df[any_change_mask].copy()\n",
        "\n",
        "        # Build the final list of columns to display in the log for clarity\n",
        "        log_columns = key_columns\n",
        "        for col in columns_to_compare:\n",
        "            log_columns.extend([f\"{col}_before\", f\"{col}_after\"])\n",
        "\n",
        "        print(\"\\n--- 🧐 Detailed Change Log (Sample) ---\")\n",
        "        print(\"Showing a sample of rows where the machine's guess was corrected by the expert.\")\n",
        "        display(change_log_df[log_columns].head(20))\n",
        "    else:\n",
        "        print(\"\\n✅ No differences found between the two files for the specified columns.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n❌ ERROR: Could not find a file. Please check the paths.\")\n",
        "    print(e)\n",
        "except Exception as e:\n",
        "    print(f\"\\n❌ An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expert modify nearly an half of the entries (47.24%)\n",
        "\n",
        "The column with the majoiryty of modification is the 'liv_name' column. This is logical becuase it was the dirty one\n",
        "\n",
        "Mathc Score in the harmonize_food_security_locations functions was set at 0.7 becuase was the best values found after several trial"
      ],
      "metadata": {
        "id": "chiFqVMxqjFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tr5FYZuA_f8"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "#\n",
        "# EXPERT CORRECTION WORKFLOW\n",
        "#\n",
        "# This script manages the integration of manual corrections from a domain expert.\n",
        "#\n",
        "# - IF 'expert.csv' exists: It loads the file, merges it with the harmonized\n",
        "#   dataset, and applies the corrections.\n",
        "# - IF 'expert.csv' does NOT exist: It creates a template CSV containing the\n",
        "#   unique, machine-harmonized rows and saves it for the expert to review.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# --- CONDITIONAL LOGIC ---\n",
        "# Check if the expert correction file already exists.\n",
        "if os.path.exists(EXPERT_FILE_PATH2):\n",
        "    # --- PATH 1: File exists, apply corrections ---\n",
        "    print(f\"✅ Expert file found at '{EXPERT_FILE_PATH2}'. Loading and applying corrections...\")\n",
        "\n",
        "    # Load the CSV containing the expert's manual corrections.\n",
        "    # index_col=0 correctly uses the first column as the DataFrame index.\n",
        "    expert_corrections = pd.read_csv(EXPERT_FILE_PATH2, index_col=0)\n",
        "\n",
        "    # Define the columns that the expert was supposed to correct.\n",
        "    # These are the columns that exist in BOTH the original and expert files.\n",
        "    columns_to_update = [\n",
        "        'liv_name', 'adm1_final', 'adm2_final',\n",
        "        'lz_code1', 'lz_code2', 'lz_code3'\n",
        "    ]\n",
        "\n",
        "    # Perform a left merge to join the corrections with the main DataFrame.\n",
        "    # 'how=left' ensures all original rows are kept.\n",
        "    # 'suffixes' helps differentiate between original and expert-provided columns.\n",
        "    # New columns from the expert file (like 'Plotting_Level') will be added automatically\n",
        "    # without any suffix.\n",
        "    df_merged = pd.merge(\n",
        "        df_food_security_cleaned2,\n",
        "        expert_corrections,\n",
        "        on=['provider_admin1_name', 'provider_admin2_name'],\n",
        "        how='left',\n",
        "        suffixes=('_original', '_expert')\n",
        "    )\n",
        "\n",
        "    # --- FIX: Explicitly ensure 'Plotting_Level' from expert_corrections is in df_merged ---\n",
        "    plotting_level_col = 'Plotting_Level'\n",
        "    if plotting_level_col in expert_corrections.columns:\n",
        "        # Merge the 'Plotting_Level' column from expert_corrections into df_merged\n",
        "        # This adds the column if it doesn't exist or updates it if it does.\n",
        "        # Use the index for merging to align correctly.\n",
        "        df_merged = df_merged.merge(expert_corrections[[plotting_level_col]], left_index=True, right_index=True, how='left')\n",
        "\n",
        "    # --- NEW SECTION: TRACK AND LOG ALL CHANGES ---\n",
        "    # This block identifies every single change made by the expert.\n",
        "    print(\"\\n--- Checking for changes introduced by the expert file ---\")\n",
        "    changes_log = []\n",
        "\n",
        "    # Check for changes in the columns expected to be updated\n",
        "    for col in columns_to_update:\n",
        "        original_col = f\"{col}_original\"\n",
        "        expert_col = f\"{col}_expert\"\n",
        "\n",
        "        # Ensure both original and expert columns exist after the merge before comparing\n",
        "        if original_col in df_merged.columns and expert_col in df_merged.columns:\n",
        "            # Create a mask to find rows where a correction was provided and it's different\n",
        "            # from the original value. This handles cases where original was NaN or different.\n",
        "            mask = (\n",
        "                df_merged[expert_col].notna() &\n",
        "                (df_merged[original_col] != df_merged[expert_col])\n",
        "            )\n",
        "\n",
        "            # Filter the rows where changes occurred for the current column\n",
        "            changed_rows = df_merged[mask]\n",
        "\n",
        "            if not changed_rows.empty:\n",
        "                # Create a temporary DataFrame logging the specific changes for this column\n",
        "                log_df = pd.DataFrame({\n",
        "                    'provider_admin1_name': changed_rows['provider_admin1_name'],\n",
        "                    'provider_admin2_name': changed_rows['provider_admin2_name'],\n",
        "                    'column_changed': col,\n",
        "                    'original_value': changed_rows[original_col],\n",
        "                    'new_value': changed_rows[expert_col]\n",
        "                })\n",
        "                changes_log.append(log_df)\n",
        "\n",
        "    # Check for changes/additions in the 'Plotting_Level' column specifically\n",
        "    # This check now happens AFTER ensuring the column is in df_merged\n",
        "    if plotting_level_col in df_merged.columns: # Check in df_merged now\n",
        "        # Find rows where Plotting_Level was provided by the expert AND is not NaN\n",
        "        mask_plotting_level = df_merged[plotting_level_col].notna()\n",
        "\n",
        "        # If an original Plotting_Level column existed before the merge (unlikely in this workflow, but for safety)\n",
        "        original_pl_col_name = f'{plotting_level_col}_original'\n",
        "        if original_pl_col_name in df_merged.columns:\n",
        "             mask_plotting_level = mask_plotting_level & (df_merged[plotting_level_col] != df_merged[original_pl_col_name])\n",
        "\n",
        "        changed_plotting_level_rows = df_merged[mask_plotting_level]\n",
        "\n",
        "        if not changed_plotting_level_rows.empty:\n",
        "            log_df_pl = pd.DataFrame({\n",
        "                'provider_admin1_name': changed_plotting_level_rows['provider_admin1_name'],\n",
        "                'provider_admin2_name': changed_plotting_level_rows['provider_admin2_name'],\n",
        "                'column_changed': plotting_level_col,\n",
        "                'original_value': df_merged.loc[changed_plotting_level_rows.index, original_pl_col_name] if original_pl_col_name in df_merged.columns else None,\n",
        "                'new_value': changed_plotting_level_rows[plotting_level_col]\n",
        "            })\n",
        "            changes_log.append(log_df_pl)\n",
        "\n",
        "\n",
        "    if changes_log:\n",
        "        # Combine all the individual change logs into a single summary DataFrame\n",
        "        df_changes_summary = pd.concat(changes_log, ignore_index=True)\n",
        "        print(f\"🔎 Found {len(df_changes_summary)} total changes. Stored in 'df_changes_summary'.\")\n",
        "        print(\"Here is a sample of the changes:\")\n",
        "        display(df_changes_summary.head())\n",
        "    else:\n",
        "        print(\"✅ No differences found between the original data and the expert file for the specified columns.\")\n",
        "        df_changes_summary = pd.DataFrame() # Create an empty df if no changes\n",
        "\n",
        "\n",
        "    # --- APPLY CORRECTIONS ---\n",
        "    # Update the columns: prioritize the expert's input, fall back to the original if null.\n",
        "    # This loop now only handles the modification of existing columns.\n",
        "    for col in columns_to_update:\n",
        "        original_col = f\"{col}_original\"\n",
        "        expert_col = f\"{col}_expert\"\n",
        "\n",
        "        # Check if the expert column exists (meaning it was in the expert file)\n",
        "        if expert_col in df_merged.columns:\n",
        "            # If it exists, prioritize the expert's value.\n",
        "            df_merged[col] = df_merged[expert_col].combine_first(df_merged[original_col])\n",
        "        # No 'else' needed here; if the expert column doesn't exist, the original column remains.\n",
        "\n",
        "\n",
        "    # The df_food_security_cleaned3 is now simply the df_merged\n",
        "    df_food_security_cleaned3 = df_merged.copy()\n",
        "\n",
        "\n",
        "    # Clean up temporary columns created by the merge.\n",
        "    # This needs to be done AFTER ensuring Plotting_Level is correctly handled.\n",
        "    cols_to_drop = [c for c in df_food_security_cleaned3.columns if c.endswith('_original') or c.endswith('_expert')]\n",
        "    df_food_security_cleaned3 = df_food_security_cleaned3.drop(columns=cols_to_drop, errors='ignore') # Use errors='ignore' to prevent errors if cols don't exist\n",
        "\n",
        "\n",
        "    print(\"\\n🚀 Expert corrections have been successfully applied.\")\n",
        "\n",
        "else:\n",
        "    # --- PATH 2: File does not exist, create it for the expert ---\n",
        "    # This part of the logic remains unchanged.\n",
        "    print(f\"⚠️ Expert file not found. Creating a new template at '{EXPERT_FILE_PATH}'...\")\n",
        "\n",
        "    selected_columns = [\n",
        "        'provider_admin1_name', 'provider_admin2_name', 'liv_name',\n",
        "        'adm1_final', 'adm2_final', 'lz_code1', 'lz_code2', 'lz_code3'\n",
        "    ]\n",
        "    expert_template = df_food_security_cleaned2[selected_columns].drop_duplicates().reset_index(drop=True)\n",
        "    # Also add the 'Plotting_Level' column to the template for the expert to fill\n",
        "    expert_template['Plotting_Level'] = None # Or a default value if appropriate\n",
        "    expert_template.to_csv(EXPERT_FILE_PATH)\n",
        "    df_food_security_cleaned3 = df_food_security_cleaned2.copy() # Assign to the same final variable name\n",
        "    # Ensure the Plotting_Level column is added here too, even if None, so the next cell doesn't error.\n",
        "    if 'Plotting_Level' not in df_food_security_cleaned3.columns:\n",
        "         df_food_security_cleaned3['Plotting_Level'] = None\n",
        "    print(\"\\n✅ Template file 'expert_reviewed.csv' has been created.\")\n",
        "    print(\"ACTION REQUIRED: Please have the expert edit this file and then re-run this cell.\")\n",
        "\n",
        "\n",
        "# --- FINAL VERIFICATION ---\n",
        "print(\"\\n--- Displaying a sample of key and corrected columns ---\")\n",
        "\n",
        "# Define the specific columns of interest for verification.\n",
        "# We add 'Plotting_Level' to ensure it was added correctly.\n",
        "columns_to_display = [\n",
        "    'provider_admin1_name',\n",
        "    'provider_admin2_name',\n",
        "    'liv_name',\n",
        "    'adm1_final',\n",
        "    'adm2_final',\n",
        "    'lz_code1',\n",
        "    'lz_code2',\n",
        "    'lz_code3',\n",
        "    'Plotting_Level' # Add the new column here for verification\n",
        "]\n",
        "\n",
        "# Filter columns_to_display to only those that actually exist in the final dataframe\n",
        "# to avoid KeyErrors if the expert file hasn't been applied yet.\n",
        "existing_cols_to_display = [col for col in columns_to_display if col in df_food_security_cleaned3.columns]\n",
        "\n",
        "# Display a sample of the final, corrected DataFrame.\n",
        "display(df_food_security_cleaned3[existing_cols_to_display].sample(n=10, random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X5VXGR5LiE7"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# FUNCTION TO APPLY EXPERT CORRECTIONS\n",
        "#\n",
        "# This function encapsulates the entire process of merging the expert-reviewed\n",
        "# corrections with the machine-harmonized dataset. It includes robust key\n",
        "# cleaning to ensure a successful merge.\n",
        "# ==============================================================================\n",
        "\n",
        "def apply_expert_corrections(main_df, expert_filepath):\n",
        "    \"\"\"\n",
        "    Cleans, merges, and applies expert corrections to the main harmonized DataFrame.\n",
        "\n",
        "    Args:\n",
        "        main_df (pd.DataFrame): The DataFrame after semantic harmonization (e.g., df_food_security_cleaned2).\n",
        "        expert_filepath (str): The file path to the expert-reviewed CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame with the expert corrections applied, ready for georeferencing.\n",
        "                      Returns None if the expert file is not found.\n",
        "    \"\"\"\n",
        "    # --- 1. PRE-CHECKS ---\n",
        "    if not os.path.exists(expert_filepath):\n",
        "        print(f\"❌ ERROR: The expert file was not found at '{expert_filepath}'. Cannot proceed.\")\n",
        "        return None\n",
        "\n",
        "    print(\"--- 🚀 Starting Final Merge with Expert Corrections ---\")\n",
        "    # Work on copies to avoid modifying the original dataframes\n",
        "    main_df_copy = main_df.copy()\n",
        "    expert_corrections = pd.read_csv(expert_filepath)\n",
        "    print(f\"✅ Loaded expert file with {len(expert_corrections)} rules.\")\n",
        "\n",
        "    # --- 2. ENHANCED KEY STANDARDIZATION ---\n",
        "    # Define merge keys\n",
        "    key_columns = ['provider_admin1_name', 'provider_admin2_name']\n",
        "\n",
        "    def standardize_text_column(series):\n",
        "        \"\"\"Applies robust cleaning to a text column.\"\"\"\n",
        "        series = series.astype(str).str.lower()\n",
        "        series = series.str.replace('nan', '', regex=False)\n",
        "        series = series.str.replace(r'[_\\-+]+', ' ', regex=True)\n",
        "        series = series.str.replace(r'\\s+', ' ', regex=True)\n",
        "        series = series.str.strip()\n",
        "        return series\n",
        "\n",
        "    print(\"Applying enhanced cleaning to merge keys in both DataFrames...\")\n",
        "    for col in key_columns:\n",
        "        if col in main_df_copy.columns:\n",
        "            main_df_copy[col] = standardize_text_column(main_df_copy[col])\n",
        "        if col in expert_corrections.columns:\n",
        "            expert_corrections[col] = standardize_text_column(expert_corrections[col])\n",
        "    print(\"✅ Keys standardized.\")\n",
        "\n",
        "    # --- 3. PERFORM THE MERGE ---\n",
        "    # Merge the corrections into the main dataframe, creating '_original' and '_expert' columns\n",
        "    df_merged = pd.merge(\n",
        "        main_df_copy,\n",
        "        expert_corrections,\n",
        "        on=key_columns,\n",
        "        how='left',\n",
        "        suffixes=('_original', '_expert')\n",
        "    )\n",
        "    print(\"✅ Merge complete.\")\n",
        "\n",
        "    # --- 4. APPLY CORRECTIONS ---\n",
        "    # Identify all columns that the expert file might provide\n",
        "    expert_cols = [col for col in expert_corrections.columns if col not in key_columns]\n",
        "\n",
        "    for col in expert_cols:\n",
        "        original_col = f\"{col}_original\"\n",
        "        expert_col_suffixed = f\"{col}_expert\"\n",
        "\n",
        "        # Check if the column exists in the merged dataframe (it should)\n",
        "        if expert_col_suffixed in df_merged.columns:\n",
        "            # Prioritize the expert's value. If the expert's value is null,\n",
        "            # keep the original machine-generated value.\n",
        "            # This correctly handles all harmonized columns and the 'Plotting_Level'.\n",
        "            df_merged[col] = df_merged[expert_col_suffixed].combine_first(df_merged.get(original_col))\n",
        "\n",
        "    # --- 5. FINALIZE THE DATAFRAME ---\n",
        "    # The final dataframe should only contain the clean, final column names\n",
        "    final_columns = list(main_df.columns) + expert_cols\n",
        "    # Remove duplicates while preserving order\n",
        "    final_columns = list(dict.fromkeys(final_columns))\n",
        "\n",
        "    # Ensure all final columns exist in the merged df before selecting\n",
        "    final_columns_exist = [col for col in final_columns if col in df_merged.columns]\n",
        "    final_df = df_merged[final_columns_exist]\n",
        "    print(\"✅ Corrections applied and final DataFrame created.\")\n",
        "\n",
        "    # --- 6. VALIDATION ---\n",
        "    null_plotting_levels = final_df['Plotting_Level'].isnull().sum()\n",
        "    total_rows = len(final_df)\n",
        "\n",
        "    print(\"\\n--- Merge Validation Report ---\")\n",
        "    if null_plotting_levels > 0:\n",
        "        print(f\"⚠️ WARNING: Merge was not fully successful for {null_plotting_levels} out of {total_rows} rows.\")\n",
        "        print(\"   This means some rows in your main data did not find a match in the expert file.\")\n",
        "    else:\n",
        "        print(f\"🎉 SUCCESS! All {total_rows} rows were successfully merged with expert rules.\")\n",
        "\n",
        "    return final_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_food_security_cleaned3 = apply_expert_corrections(df_food_security_cleaned2, EXPERT_FILE_PATH2)\n",
        "print(\"\\n--- Final DataFrame Ready ---\")\n",
        "display(df_food_security_cleaned3.head())\n",
        "print(df_food_security_cleaned3.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4hp6hWiQmkI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# PLOTTING LEVEL-DRIVEN GEOREFERENCING AND VISUALIZATION (REVISED)\n",
        "#\n",
        "# # --- Imports and Configuration ---\n",
        "# Ensure tqdm is ready for pandas integration\n",
        "tqdm.pandas()\n",
        "\n",
        "# CRS codes for consistent projection\n",
        "target_crs_projected = 'EPSG:32618'\n",
        "target_crs_plotting = 'EPSG:4326'\n",
        "\n",
        "\n",
        "# --- PHASE 1: PREPARATION AND SETUP -------------------------------------------\n",
        "print(\"--- Step 1: Reprojecting Source GeoDataFrames ---\")\n",
        "\n",
        "# Reproject all source GDFs to the target projected CRS for accurate analysis\n",
        "try:\n",
        "    gdf_admin0_proj = gdf_admin0.to_crs(target_crs_projected)\n",
        "    gdf_admin1_proj = gdf_admin1.to_crs(target_crs_projected)\n",
        "    gdf_admin2_proj = gdf_admin2.to_crs(target_crs_projected)\n",
        "    gdf_livelihood_proj = gdf_livelihood.to_crs(target_crs_projected)\n",
        "    print(\"✅ Source geodataframes reprojected successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during reprojection: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n--- Step 2: Creating Geometry Lookups and National Boundary ---\")\n",
        "\n",
        "# Create Python dictionaries for fast geometry retrieval\n",
        "geom_map_adm1 = gdf_admin1_proj.set_index('adm1_name_en')['geometry'].to_dict()\n",
        "geom_map_adm2 = gdf_admin2_proj.set_index('adm2_name_en')['geometry'].to_dict()\n",
        "\n",
        "# Pre-calculate the national boundary. Using .union_all() as .unary_union is deprecated.\n",
        "haiti_boundary = gdf_admin0_proj.union_all()\n",
        "print(\"✅ Lookup maps and national boundary created.\")\n",
        "\n",
        "\n",
        "# --- PHASE 2: SPLIT-APPLY-COMBINE WITH PROGRESS BARS --------------------------\n",
        "\n",
        "print(\"\\n--- Step 3: Applying Vectorized Georeferencing Logic ---\")\n",
        "# Make a copy of the main dataframe to work on\n",
        "df = df_food_security_cleaned3.copy()\n",
        "\n",
        "# -----------------\n",
        "# 2.1: SPLIT\n",
        "# -----------------\n",
        "df_level_1 = df[df['Plotting_Level'] == 1].copy()\n",
        "df_level_2 = df[df['Plotting_Level'] == 2].copy()\n",
        "df_level_3 = df[df['Plotting_Level'] == 3].copy()\n",
        "df_level_4 = df[df['Plotting_Level'] == 4].copy()\n",
        "print(f\"   - Data split into {len(df_level_1)} (L1), {len(df_level_2)} (L2), {len(df_level_3)} (L3), {len(df_level_4)} (L4) rows.\")\n",
        "\n",
        "# -----------------\n",
        "# 2.2: APPLY\n",
        "# -----------------\n",
        "from shapely.geometry.base import BaseGeometry\n",
        "\n",
        "# --- Process Level 1: National (Instantaneous, no progress bar needed) ---\n",
        "if not df_level_1.empty:\n",
        "    df_level_1['geometry'] = haiti_boundary\n",
        "    df_level_1['geometry_status'] = 'Level 1: OK (National)'\n",
        "    print(\"   - Level 1 processed.\")\n",
        "\n",
        "# --- Process Level 2: Department (Map is very fast, no progress bar needed) ---\n",
        "if not df_level_2.empty:\n",
        "    df_level_2['geometry'] = df_level_2['adm1_final'].map(geom_map_adm1)\n",
        "    df_level_2['geometry_status'] = df_level_2['geometry'].apply(\n",
        "        lambda g: 'Level 2: OK (Department)' if g else 'Level 2: FAILED (ADM1 name not found)'\n",
        "    )\n",
        "    print(\"   - Level 2 processed.\")\n",
        "\n",
        "# --- Process Level 3: Department(s) (with tqdm) ---\n",
        "if not df_level_3.empty:\n",
        "    geoms1 = df_level_3['adm1_final'].map(geom_map_adm1)\n",
        "    geoms2 = df_level_3['adm1_1_final'].map(geom_map_adm1)\n",
        "\n",
        "    # --- TQDM ADDED ---\n",
        "    # Wrap the zipped iterables with tqdm to show progress on the geometry union.\n",
        "    df_level_3['geometry'] = [\n",
        "        g1.union(g2) if isinstance(g1, BaseGeometry) and isinstance(g2, BaseGeometry)\n",
        "        else g1 if isinstance(g1, BaseGeometry)\n",
        "        else None\n",
        "        for g1, g2 in tqdm(zip(geoms1, geoms2), total=len(df_level_3), desc=\"Processing Level 3\")\n",
        "    ]\n",
        "    df_level_3['geometry_status'] = df_level_3['geometry'].apply(\n",
        "        lambda g: 'Level 3: OK (Department Boundary)' if g else 'Level 3: FAILED (ADM1 name(s) not found)'\n",
        "    )\n",
        "    print(\"   - Level 3 processed.\")\n",
        "\n",
        "# --- Process Level 4: Commune(s) (with tqdm) ---\n",
        "if not df_level_4.empty:\n",
        "    geoms1 = df_level_4['adm2_final'].map(geom_map_adm2)\n",
        "    geoms2 = df_level_4['adm2_1_final'].map(geom_map_adm2)\n",
        "\n",
        "    # --- TQDM ADDED ---\n",
        "    # Apply the same tqdm wrapper for Level 4 processing.\n",
        "    df_level_4['geometry'] = [\n",
        "        g1.union(g2) if isinstance(g1, BaseGeometry) and isinstance(g2, BaseGeometry)\n",
        "        else g1 if isinstance(g1, BaseGeometry)\n",
        "        else None\n",
        "        for g1, g2 in tqdm(zip(geoms1, geoms2), total=len(df_level_4), desc=\"Processing Level 4\")\n",
        "    ]\n",
        "    df_level_4['geometry_status'] = df_level_4['geometry'].apply(\n",
        "        lambda g: 'Level 4: OK (Commune Boundary)' if g else 'Level 4: FAILED (ADM2 name(s) not found)'\n",
        "    )\n",
        "    print(\"   - Level 4 processed.\")\n",
        "\n",
        "# -----------------\n",
        "# 2.3: COMBINE\n",
        "# -----------------\n",
        "processed_dfs = [df_level_1, df_level_2, df_level_3, df_level_4]\n",
        "df_combined = pd.concat(processed_dfs)\n",
        "df_combined.sort_index(inplace=True)\n",
        "\n",
        "df_food_security_final = gpd.GeoDataFrame(\n",
        "    df_combined, geometry='geometry', crs=target_crs_projected\n",
        ")\n",
        "print(\"✅ Georeferencing complete and results combined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PHASE 4: VISUAL MAPPING ANALYSIS ---\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"--- 🗺️  Generating Maps for each Plotting Level ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- Step 4.1: Prepare Data for Plotting ---\n",
        "# Reproject the final geodataframe and the necessary layers to the plotting CRS (EPSG:4326)\n",
        "# This is required for compatibility with web map tiles from contextily.\n",
        "print(\"   - Reprojecting data to plotting CRS (EPSG:4326)...\")\n",
        "df_plot = df_food_security_final.to_crs(target_crs_plotting)\n",
        "gdf_livelihood_plot = gdf_livelihood_proj.to_crs(target_crs_plotting)\n",
        "# Use the ADM1 layer as a consistent grey basemap for context\n",
        "haiti_basemap_plot = gdf_admin1_proj.to_crs(target_crs_plotting)\n",
        "print(\"✅ Data ready for plotting.\")\n",
        "\n",
        "\n",
        "# --- Step 4.2: Create the Plotting Grid ---\n",
        "# Set up a 2x2 subplot grid to display one map for each level.\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 18))\n",
        "axes = axes.flatten() # Flatten the 2x2 array into a 1D array for easy iteration\n",
        "fig.suptitle('Georeferenced Areas by Plotting Level with LZ Overlay', fontsize=22, y=0.95)\n",
        "\n",
        "\n",
        "# --- Step 4.3: Iterate and Generate Each Map ---\n",
        "for level in range(1, 5):\n",
        "    ax = axes[level-1] # Select the correct subplot for the current level\n",
        "\n",
        "    # Plot the grey administrative basemap first to provide context\n",
        "    haiti_basemap_plot.plot(ax=ax, color='#E0E0E0', edgecolor='white', linewidth=0.5)\n",
        "\n",
        "    # Filter the geodataframe to get only the rows for the current level\n",
        "    level_gdf = df_plot[df_plot['Plotting_Level'] == level].copy()\n",
        "    # Ensure we only plot valid geometries\n",
        "    level_gdf.dropna(subset=['geometry'], inplace=True)\n",
        "\n",
        "    ax.set_title(f'Plotting Level {level} ({len(level_gdf)} areas found)', fontsize=14)\n",
        "\n",
        "    if not level_gdf.empty:\n",
        "        # Plot the primary georeferenced areas (national, department, or commune boundaries)\n",
        "        level_gdf.plot(ax=ax, alpha=0.5, edgecolor='black', linewidth=0.7, color='cyan')\n",
        "\n",
        "        # For levels 3 and 4, we add the Livelihood Zone overlay\n",
        "        if level in [3, 4]:\n",
        "            # Create a single dissolved geometry for all areas in the current level.\n",
        "            # This is highly efficient for clipping.\n",
        "            level_union_geom = level_gdf.union_all()\n",
        "\n",
        "            # Spatially query to find all LZs that intersect this combined area.\n",
        "            intersecting_lzs = gdf_livelihood_plot[gdf_livelihood_plot.intersects(level_union_geom)]\n",
        "\n",
        "            if not intersecting_lzs.empty:\n",
        "                # Clip the intersecting LZs to the exact boundary of the level's geometry.\n",
        "                # This ensures LZs are only shown where they are relevant.\n",
        "                clipped_lzs = gpd.clip(intersecting_lzs, level_union_geom)\n",
        "\n",
        "                # Plot the clipped and colored LZs on top\n",
        "                clipped_lzs.plot(\n",
        "                    ax=ax,\n",
        "                    alpha=0.7,\n",
        "                    cmap='viridis', # Use a colormap to distinguish different LZs\n",
        "                    edgecolor='white',\n",
        "                    linewidth=0.5\n",
        "                )\n",
        "    else:\n",
        "        # If no data exists for a level, display a message on the plot\n",
        "        ax.text(0.5, 0.5, 'No valid geometries for this level',\n",
        "                horizontalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "\n",
        "    # Add a web basemap tile layer for geographic context\n",
        "    ctx.add_basemap(ax, crs=haiti_basemap_plot.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "print(\"✅ Maps generated.\")\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.93]) # Adjust layout to make space for the suptitle\n",
        "plt.show()\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "2r4k9qGDJp3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD1sSLU1YgMF"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Conflict\n",
        "\n",
        "A weekly ACLED dataset providing the total number of reported political violence, civilian-targeting, and demonstration events in Haiti. Note: These are aggregated data files organized by country-year and country-month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tjv9Zl81Yn4D"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: RAW DATA PRELIMINARY INSPECTION for `conflict`\n",
        "# ==============================================================================\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `conflict`: Structure and Memory Footprint ---\")\n",
        "loaded_data['conflict'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `conflict`: Initial Rows Sample ---\")\n",
        "display(loaded_data['conflict'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `conflict`: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['conflict'].shape[0]} rows and {loaded_data['conflict'].shape[1]} columns.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc1E7WL6ZE53"
      },
      "source": [
        "The dataset is very clean. There missing values in fatalities column, but this is logic becuase we can expect conflict event without affected people or fatalities\n",
        "\n",
        "Two time columns, reference_period_start and reference_period_end\n",
        "\n",
        "The first row is composed by metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkTWvqWHZhD9"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURED EXECUTION of inspect_and_clean_data for `conflict`\n",
        "# ==============================================================================\n",
        "\n",
        "# Now, we call the function with informed parameters.\n",
        "df_conflict_cleaned = inspect_and_clean_data(\n",
        "    df=loaded_data['conflict'],\n",
        "    df_name=\"conflict\",\n",
        "    remove_hdx_header=True,\n",
        "    date_columns=['reference_period_start', 'reference_period_end']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIZqNZd1kHP4"
      },
      "outputs": [],
      "source": [
        "#===============================================================================\n",
        "# FEATURE ENGINEERING & HARMONIZATION for `conflict`\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 3. GEOSPATIAL HARMONIZATION USING A CANONICAL SOURCE ---\n",
        "# Objective: Enrich the data with official P-codes from the Geodatabase.\n",
        "# Assumption: The 'admin2_name' column in this dataset contains names that\n",
        "# are clean enough for a direct merge with the canonical location table.\n",
        "print(\"\\n--- Geospatial Harmonization Report for `conflict` ---\")\n",
        "\n",
        "# --- 2. Call the harmonization function ---\n",
        "# Now the 'georef_layers' variable exists and can be used.\n",
        "df_conflict_final = harmonize_and_georeference(\n",
        "    df_to_harmonize=df_conflict_cleaned,\n",
        "    georef_layers=georef_layers,\n",
        "    georef_key='gdf_adm2', # Specify which layer to use for the join\n",
        "    source_col='admin2_name',\n",
        "    canonical_col='adm2_name_en' # Make sure this is the correct column name in gdf_adm2\n",
        ")\n",
        "\n",
        "# --- 3. Display the result ---\n",
        "print(\"\\\\n--- Preview of the final georeferenced food price data ---\")\n",
        "display(df_conflict_final .head())\n",
        "print(f\"CRS of the final GeoDataFrame: {df_conflict_final .crs}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCiBRiCQlbpB"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Operation Presence\n",
        "\n",
        "\n",
        "**Source**: Latest 3W (Who, What, Where) analysis from OCHA.\n",
        "\n",
        "**Methodological Note**: This dataset intentionally represents a **current snapshot** of humanitarian presence. Past data is excluded to focus the analysis on currently active organizations, ensuring all insights are actionable and relevant for present strategic planning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd6xCILQmjdz"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: RAW DATA PRELIMINARY INSPECTION for `conflict`\n",
        "# ==============================================================================\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `conflict`: Structure and Memory Footprint ---\")\n",
        "loaded_data['operational_presence'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `conflict`: Initial Rows Sample ---\")\n",
        "display(loaded_data['operational_presence'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `conflict`: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['operational_presence'].shape[0]} rows and {loaded_data['operational_presence'].shape[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5v9d2dInJ-2"
      },
      "source": [
        "The dataset have no missing values. We proceed removing the header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR8NqcVknQIp"
      },
      "outputs": [],
      "source": [
        "#===============================================================================\n",
        "# CONFIGURED EXECUTION of inspect_and_clean_data for `operational_presence`\n",
        "# ==============================================================================\n",
        "# Analysis from Preliminary Inspection:\n",
        "# 1. The first row is confirmed to be an HDX metadata header. `remove_hdx_header=True`.\n",
        "# 2. The DtypeWarning at load time suggests mixed data types. The function will\n",
        "#    attempt to handle numeric conversions.\n",
        "# 3. We will let the function auto-detect date columns.\n",
        "\n",
        "# Now, we call the function with informed parameters.\n",
        "df_operational_presence_cleaned = inspect_and_clean_data(\n",
        "    df=loaded_data['operational_presence'],\n",
        "    df_name=\"operational_presence\",\n",
        "    remove_hdx_header=True,\n",
        "    date_columns=['reference_period_start', 'reference_period_end']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lH7GFQIoAkc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- 3. GEOSPATIAL HARMONIZATION USING A CANONICAL SOURCE ---\n",
        "# Objective: Enrich the data with official P-codes from the Geodatabase.\n",
        "# Assumption: The 'admin2_name' column is the correct join key.\n",
        "print(\"\\n--- Geospatial Harmonization Report for `operational_presence` ---\")\n",
        "\n",
        "# --- 2. Call the harmonization function ---\n",
        "# Now the 'georef_layers' variable exists and can be used.\n",
        "df_operational_presence_final = harmonize_and_georeference(\n",
        "    df_to_harmonize=df_operational_presence_cleaned,\n",
        "    georef_layers=georef_layers,\n",
        "    georef_key='gdf_adm2', # Specify which layer to use for the join\n",
        "    source_col='admin2_name',\n",
        "    canonical_col='adm2_name_en' # Make sure this is the correct column name in gdf_adm2\n",
        ")\n",
        "\n",
        "# --- 3. Display the result ---\n",
        "print(\"\\\\n--- Preview of the final georeferenced food price data ---\")\n",
        "display(df_operational_presence_final .head())\n",
        "print(f\"CRS of the final GeoDataFrame: {df_operational_presence_final .crs}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcgYAMbApkKj"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Humanitarian needs\n",
        "\n",
        "This dataset contains the overall people in need and intersectoral severity by disaggregation level which Includes administrative divisions and population groups, depending on each country's decision. The dataset is produced by the United Nations for the Coordination of Humanitarian Affairs (OCHA) in collaboration with humanitarian partners using the Joint Intersectoral Analysis Framework(JIAF)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UoMOyyUpudZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: RAW DATA PRELIMINARY INSPECTION for `Humanitarian Needs`\n",
        "# ==============================================================================\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `Humanitarian Needs`: Structure and Memory Footprint ---\")\n",
        "loaded_data['humanitarian_needs'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `Humanitarian Needs`: Initial Rows Sample ---\")\n",
        "display(loaded_data['humanitarian_needs'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `Humanitarian Needs: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['humanitarian_needs'].shape[0]} rows and {loaded_data['humanitarian_needs'].shape[1]} columns.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset seems to be very clean but head() function shows many NaN values.\n",
        "\n",
        "provider_admin1_name has only 1 value\n",
        "\n",
        "The first row is composed by meatadata"
      ],
      "metadata": {
        "id": "BOQg_BLKt5YR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw_JT5YNqcIh"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CONFIGURED EXECUTION of inspect_and_clean_data for `humanitarian_needs`\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# Now, we call the function with explicit, informed parameters.\n",
        "df_humanitarian_needs_cleaned = inspect_and_clean_data(\n",
        "    df=loaded_data['humanitarian_needs'],\n",
        "    df_name=\"humanitarian_needs\",\n",
        "    remove_hdx_header=True,\n",
        "    date_columns=['reference_period_start', 'reference_period_end'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The missing values analysis now show a clear picture. A deeper analysis will be conducted in the Data Cleaning Section.\n",
        "\n",
        "Some stardisation is need in the categoru colomn"
      ],
      "metadata": {
        "id": "gQTHNcokukpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr6zxVMtrVpL"
      },
      "outputs": [],
      "source": [
        "\n",
        "## --- 3. DATA CLEANING: STANDARDIZING CATEGORICAL VALUES ---\n",
        "# Objective: Map French and ambiguous category names to a consistent English standard.\n",
        "category_mask = {\n",
        "    'Femmes': 'Adult - Female',\n",
        "    'Hommes': 'Adult - Male',\n",
        "    'Filles': 'Children - Female',\n",
        "    'Garcons': 'Children - Male',\n",
        "    'Pers. agees': 'Elderly',\n",
        "    'Autres': 'Other',\n",
        "    'CommunauteHote': 'Host Communities',\n",
        "    'PDI': 'IDP', # Personnes Déplacées Internes -> Internally Displaced Person\n",
        "    'Migrants/deportes': 'Migrants'\n",
        "}\n",
        "\n",
        "# Apply the mapping to standardize the 'category' column\n",
        "df_humanitarian_needs_cleaned['category'] = df_humanitarian_needs_cleaned['category'].replace(category_mask)\n",
        "\n",
        "# Display the value counts after cleaning to verify the result\n",
        "print(\"\\n--- 'category' column after standardization ---\")\n",
        "print(df_humanitarian_needs_cleaned['category'].value_counts())\n",
        "\n",
        "# --- 4. GEOSPATIAL HARMONIZATION USING A CANONICAL SOURCE ---\n",
        "# Objective: Enrich the data with official P-codes from the Geodatabase.\n",
        "# NOTE: This dataset may require a manual/hybrid cleaning approach for its\n",
        "# location names, similar to the one developed for `df_food_security`.\n",
        "\n",
        "df_humanitarian_needs_final = harmonize_and_georeference(\n",
        "    df_to_harmonize=df_humanitarian_needs_cleaned,\n",
        "    georef_layers=georef_layers,\n",
        "    georef_key='gdf_adm2', # Specify which layer to use for the join\n",
        "    source_col='admin2_name',\n",
        "    canonical_col='adm2_name_en' # Make sure this is the correct column name in gdf_adm2\n",
        ")\n",
        "\n",
        "# --- 3. Display the result ---\n",
        "print(\"\\\\n--- Preview of the final georeferenced food price data ---\")\n",
        "display(df_humanitarian_needs_final.head())\n",
        "print(f\"CRS of the final GeoDataFrame: {df_humanitarian_needs_final.crs}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_operational_presence_final = harmonize_and_georeference(\n",
        "    df_to_harmonize=df_operational_presence_cleaned,\n",
        "    georef_layers=georef_layers,\n",
        "    georef_key='gdf_adm2',\n",
        "    source_col='admin2_name',\n",
        "    canonical_col='adm2_name_en'\n",
        ")\n",
        "\n",
        "# --- 3. Display the result ---\n",
        "print(\"\\\\n--- Preview of the final georeferenced food price data ---\")\n",
        "display(df_operational_presence_final .head())\n",
        "print(f\"CRS of the final GeoDataFrame: {df_operational_presence_final .crs}\")"
      ],
      "metadata": {
        "id": "-8sQfNTwXrXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUqpKb2JopB3"
      },
      "source": [
        "## Loading and Georeferencing Dataset: Relative Wealth Index (RWI)\n",
        "\n",
        "**Source**: Meta Data for Good (data via HDX), basato su dati del 2021.\n",
        "\n",
        "**Methodological Note**: This dataset is used as a proxy for **structural socio-economic vulnerability**. While the data is from 2021, the RWI is based on slow-changing asset indicators (e.g., infrastructure, housing quality) rather than volatile income. It is therefore considered a robust baseline for identifying historically disadvantaged areas that are likely to have lower resilience to current shocks. The primary limitation is that this index does not capture wealth distribution shifts caused by post-2021 events, such as widespread gang violence and internal displacement. All conclusions drawn from this data will be framed in the context of pre-existing vulnerability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ToSS5uwowUl"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# STEP 1: RAW DATA PRELIMINARY INSPECTION for `relative_wealth_index`\n",
        "# ==============================================================================\n",
        "# Conduct a high-level sanity check on the raw DataFrame before any\n",
        "# transformation.\n",
        "\n",
        "print(\"--- [RAW] `relative_wealth_index`: Structure and Memory Footprint ---\")\n",
        "loaded_data['relative_wealth_index'].info()\n",
        "\n",
        "print(\"\\n--- [RAW] `relative_wealth_index`: Initial Rows Sample ---\")\n",
        "display(loaded_data['relative_wealth_index'].head())\n",
        "\n",
        "print(f\"\\n--- [RAW] `relative_wealth_index`: Dimensions ---\")\n",
        "print(f\"The raw dataset contains {loaded_data['relative_wealth_index'].shape[0]} rows and {loaded_data['relative_wealth_index'].shape[1]} columns.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK61QWKGy10z"
      },
      "outputs": [],
      "source": [
        "# ---  GEOSPATIAL HARMONIZATION USING A CANONICAL SOURCE ---\n",
        "# Convert the Relative Wealth dataframe into a GeoDataFrame.\n",
        "# The geometry is created from its OWN latitude and longitude columns.\n",
        "df_wealth_geo = gpd.GeoDataFrame(\n",
        "    loaded_data['relative_wealth_index'],\n",
        "    geometry=gpd.points_from_xy(\n",
        "        # CORRECT: Use longitude and latitude from the wealth index dataframe itself.\n",
        "        loaded_data['relative_wealth_index'].longitude,\n",
        "        loaded_data['relative_wealth_index'].latitude\n",
        "    ),\n",
        "    crs=\"EPSG:4326\"  # Set the initial Coordinate Reference System to WGS84\n",
        ")\n",
        "\n",
        "# We select only the necessary columns for a clean join.\n",
        "canonical_polygons = gdf_admin3[[\n",
        "    'adm3_name_en', 'adm2_name_en',\n",
        "    'Pcode3', 'Pcode2', 'geometry' # Corrected column names\n",
        "]]\n",
        "\n",
        "# --- PERFORM THE SPATIAL JOIN ---\n",
        "# Use gpd.sjoin to find which administrative polygon each wealth data point falls into.\n",
        "print(\"🚀 Performing Spatial Join...\")\n",
        "df_wealth_final = gpd.sjoin(\n",
        "    df_wealth_geo,\n",
        "    canonical_polygons,\n",
        "    how=\"inner\",\n",
        "    predicate='within'\n",
        ")\n",
        "\n",
        "n_original = len(df_wealth_geo)\n",
        "n_joined = len(df_wealth_final)\n",
        "n_dropped = n_original - n_joined\n",
        "pct_joined = (n_joined / n_original) * 100 if n_original > 0 else 0\n",
        "pct_dropped = (n_dropped / n_original) * 100 if n_original > 0 else 0\n",
        "\n",
        "print(f\"Total points in original dataset: {n_original}\")\n",
        "print(f\"✅ Points successfully joined to a polygon: {n_joined} ({pct_joined:.2f}%)\")\n",
        "print(f\"❌ Points NOT joined (dropped): {n_dropped} ({pct_dropped:.2f}%)\")\n",
        "\n",
        "\n",
        "# --- Step 3: Identify and Analyze Unmatched Points ---\n",
        "print(\"\\n\\n\" + \"=\"*50)\n",
        "print(\"--- 🔬 Analysis of Unmatched Points ---\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if n_dropped > 0:\n",
        "    # Identify the indices of the rows that were successfully joined\n",
        "    joined_indices = df_wealth_final.index\n",
        "\n",
        "    # Get the rows from the original dataframe that do NOT have their index in the joined result\n",
        "    unmatched_df = df_wealth_geo[~df_wealth_geo.index.isin(joined_indices)]\n",
        "\n",
        "    print(f\"Found {len(unmatched_df)} points that could not be matched to any polygon.\")\n",
        "    print(\"\\n--- Primary Reason for Failure ---\")\n",
        "    print(\"Points are dropped when their coordinates fall outside of ALL polygons in the canonical dataset.\")\n",
        "    print(\"This can happen if the points are located in the ocean, in a neighboring country, or if the polygon file has gaps.\")\n",
        "    print(\"It is also critical to ensure both GeoDataFrames have the exact same Coordinate Reference System (CRS) before the join.\\n\")\n",
        "\n",
        "    print(\"--- List of Unmatched Points (Sample) ---\")\n",
        "    # Display relevant columns: coordinates and wealth index data\n",
        "    display(unmatched_df[['latitude', 'longitude', 'rwi', 'error']].head(10))\n",
        "else:\n",
        "    print(\"🎉 Excellent! All points were successfully matched to a polygon.\")\n",
        "    print(\"No unmatched data to report.\")\n",
        "\n",
        "\n",
        "# --- Display a sample of the enriched data ---\n",
        "\n",
        "print(\"\\n--- Sample of Harmonized Wealth Data ---\")\n",
        "display(df_wealth_final[[\n",
        "    'rwi', 'error', 'adm3_name_en', 'adm3_name_en', 'Pcode3', 'Pcode3' # Added Pcode columns to display\n",
        "]].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Step 1: Load the coordinate data into a DataFrame ---\n",
        "# This uses the exact data you provided in your last message.\n",
        "print(\"   - Loading coordinate data...\")\n",
        "data_string = \"\"\"latitude,longitude,rwi,error\n",
        "18.531700,-73.509522,0.357,0.433\n",
        "18.177169,-72.586670,-0.056,0.315\n",
        "18.490028,-73.267822,0.431,0.378\n",
        "18.093644,-71.729736,-0.667,0.327\n",
        "18.072757,-73.685303,-0.231,0.334\n",
        "18.552532,-72.388916,1.156,0.438\n",
        "19.983674,-72.630615,-0.414,0.321\n",
        "19.425153,-72.696533,-0.129,0.372\n",
        "19.901053,-72.630615,0.669,0.426\n",
        "18.552532,-72.432861,1.257,0.4\n",
        "\"\"\"\n",
        "data_io = io.StringIO(data_string)\n",
        "points_df = pd.read_csv(data_io)\n",
        "\n",
        "# --- Step 2: Convert the points to a GeoDataFrame ---\n",
        "# This creates a spatial object that can be mapped.\n",
        "# We assume the coordinates are in the standard WGS84 format (EPSG:4326).\n",
        "print(\"   - Creating GeoDataFrame from points...\")\n",
        "points_gdf = gpd.GeoDataFrame(\n",
        "    points_df,\n",
        "    geometry=gpd.points_from_xy(points_df.longitude, points_df.latitude),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# --- Step 3: Prepare the Basemap and Plot ---\n",
        "# We use the 'gdf_admin1_proj' layer created earlier as the shape of Haiti.\n",
        "# Both layers must be in the same CRS (EPSG:4326) to work with web maps.\n",
        "print(\"   - Preparing map layers...\")\n",
        "haiti_basemap = gdf_admin1_proj.to_crs(epsg=4326)\n",
        "points_to_plot = points_gdf.to_crs(epsg=4326)\n",
        "\n",
        "# Create the plot figure\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
        "ax.set_title(\"Location of Unmatched Data Points in Haiti\", fontsize=16)\n",
        "\n",
        "# --- Step 4: Plot the Data ---\n",
        "# First, plot the shape of Haiti to serve as a background.\n",
        "haiti_basemap.plot(ax=ax, color='#E0E0E0', edgecolor='white', linewidth=0.5)\n",
        "\n",
        "# Next, plot the individual points on top of the map.\n",
        "points_to_plot.plot(ax=ax, color='red', marker='o', markersize=50, label='Unmatched Points')\n",
        "\n",
        "# Add a web map tile for geographic context (like roads and city names)\n",
        "print(\"   - Adding web basemap...\")\n",
        "ctx.add_basemap(ax, crs=haiti_basemap.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
        "ax.set_axis_off()\n",
        "ax.legend()\n",
        "\n",
        "# --- Step 5: Display the Map ---\n",
        "print(\"✅ Map generation complete.\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rxsjkAEGgy8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCmcYNN_7tlz"
      },
      "source": [
        "# DATA CLEANING\n",
        "\n",
        "This step is crucial to avoid misinterpreting the results of the Exploratory Analysis, especially when outliers are present, and to prevent duplicates and null values from corrupting the analysis.\n",
        "\n",
        "NOTE:\n",
        "No univariate and multivariate analysis will be performed in this section. In the EDA phase every single dataset will be analyzed and after the joining of all dataset, a complete EDA will be performed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Data Cleaning Functions"
      ],
      "metadata": {
        "id": "VFuuCQNgNQn_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W036mR3_CxDa"
      },
      "outputs": [],
      "source": [
        "def time_series_transform(df, column, add_progressive_quarter=True):\n",
        "  \"\"\"\n",
        "  Transforms a datetime column into separate month, year, and quarter columns.\n",
        "  Optionally adds a progressive quarter count that does not reset each year.\n",
        "\n",
        "  Args:\n",
        "    df (pd.DataFrame): The input DataFrame.\n",
        "    column (str): The name of the column to transform (must be datetime).\n",
        "    add_progressive_quarter (bool): If True, adds a 'progressive_quarter' column.\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: The DataFrame with added time-based features.\n",
        "  \"\"\"\n",
        "  # Create a copy to avoid modifying the original DataFrame\n",
        "  df_out = df.copy()\n",
        "\n",
        "  # Ensure the column is in datetime format\n",
        "  if not pd.api.types.is_datetime64_any_dtype(df_out[column]):\n",
        "      df_out[column] = pd.to_datetime(df_out[column], errors='coerce')\n",
        "\n",
        "  # Create standard time features\n",
        "  df_out['month'] = df_out[column].dt.month\n",
        "  df_out['year'] = df_out[column].dt.year\n",
        "\n",
        "  # --- FIX APPLIED HERE ---\n",
        "  # Use the 'column' variable instead of the hardcoded string 'column'\n",
        "  df_out['quarter'] = df_out[column].dt.quarter\n",
        "\n",
        "  # --- CRITICAL ADDITION: Add a progressive quarter if requested ---\n",
        "  if add_progressive_quarter:\n",
        "    # Find the first year in the dataset to use as a baseline\n",
        "    first_year = df_out['year'].min()\n",
        "\n",
        "    # Calculate the progressive quarter\n",
        "    # (Year difference from start * 4) + current quarter number\n",
        "    df_out['progressive_quarter'] = (df_out['year'] - first_year) * 4 + df_out['quarter']\n",
        "\n",
        "  return df_out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTz7sgA7Rvkt"
      },
      "outputs": [],
      "source": [
        "class OutlierAnalyzer:\n",
        "    \"\"\"\n",
        "    A class for detecting outliers in a dataframe.\n",
        "    Provides a choice between robust (MAD), classic (IQR), and time series decomposition methods,\n",
        "    with added capabilities for grouping.\n",
        "    NOTE: All required libraries are assumed to be installed.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, time_col=None, group_by_cols=None, method='mad',\n",
        "                 ts_index_col=None, mod_z_score_threshold=3.5, iqr_range=1.5,\n",
        "                 ts_model='additive', ts_period=12):\n",
        "        \"\"\"\n",
        "        Initialize the OutlierAnalyzer.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame to analyze.\n",
        "        time_col (str, optional): Column for simple temporal grouping (e.g., 'year'). Used by MAD and IQR.\n",
        "        group_by_cols (list, optional): Columns for categorical grouping. Used by all methods.\n",
        "        method (str): The method to use: 'mad', 'iqr', or 'ts_decompose'.\n",
        "        ts_index_col (str, optional): The specific column to be used as the datetime index for time series decomposition. Required for 'ts_decompose'.\n",
        "        mod_z_score_threshold (float): Threshold for the MAD method.\n",
        "        iqr_range (float): Multiplier for the IQR range for the IQR method and ts_decompose residuals.\n",
        "        ts_model (str): The model for seasonal_decompose ('additive' or 'multiplicative').\n",
        "        ts_period (int): The period of the time series (e.g., 12 for monthly data).\n",
        "        \"\"\"\n",
        "        self.df = df.copy() # Work on a copy to avoid modifying the original df\n",
        "        self.time_col = time_col\n",
        "        self.group_by_cols = group_by_cols\n",
        "        self.method = method\n",
        "        self.ts_index_col = ts_index_col\n",
        "        self.mod_z_score_threshold = mod_z_score_threshold\n",
        "        self.iqr_range = iqr_range\n",
        "        self.ts_model = ts_model\n",
        "        self.ts_period = ts_period\n",
        "\n",
        "        # --- Validation for method and required columns ---\n",
        "        if self.method == 'ts_decompose':\n",
        "            if not self.ts_index_col:\n",
        "                raise ValueError(\"The 'ts_index_col' parameter is required for the 'ts_decompose' method.\")\n",
        "            if not self.group_by_cols:\n",
        "                raise ValueError(\"The 'group_by_cols' parameter is required for the 'ts_decompose' method.\")\n",
        "\n",
        "        # Determine which columns are for grouping\n",
        "        all_grouping_cols = []\n",
        "        if self.time_col:\n",
        "            all_grouping_cols.append(self.time_col)\n",
        "        if self.ts_index_col:\n",
        "             all_grouping_cols.append(self.ts_index_col)\n",
        "        if self.group_by_cols:\n",
        "            all_grouping_cols.extend(self.group_by_cols)\n",
        "\n",
        "        # Identify numerical columns to be analyzed\n",
        "        self.num_cols = [\n",
        "            col for col in df.select_dtypes(include=np.number).columns\n",
        "            if col not in all_grouping_cols\n",
        "        ]\n",
        "\n",
        "    def detect_outliers(self):\n",
        "        \"\"\"Detect outliers for each numerical column using the selected method.\"\"\"\n",
        "        print(f\"--- Starting Outlier Detection (Method: {self.method.upper()}) ---\")\n",
        "\n",
        "        if self.method == 'mad' or self.method == 'iqr':\n",
        "            self._detect_with_transform()\n",
        "        elif self.method == 'ts_decompose':\n",
        "            self._detect_with_decomposition()\n",
        "        else:\n",
        "            raise ValueError(\"Method must be one of 'mad', 'iqr', or 'ts_decompose'.\")\n",
        "\n",
        "        print(\"✅ Outlier detection complete.\")\n",
        "        return self.df\n",
        "\n",
        "    def _detect_with_transform(self):\n",
        "        \"\"\"Handles outlier detection for MAD and IQR methods using groupby.transform.\"\"\"\n",
        "        all_grouping_cols = []\n",
        "        if self.time_col:\n",
        "            all_grouping_cols.append(self.time_col)\n",
        "        if self.group_by_cols:\n",
        "            all_grouping_cols.extend(self.group_by_cols)\n",
        "\n",
        "        outlier_func = self._find_outliers_mad if self.method == 'mad' else self._find_outliers_iqr\n",
        "\n",
        "        if all_grouping_cols:\n",
        "            print(f\"Grouping analysis by columns: {all_grouping_cols}\")\n",
        "            for col in self.num_cols:\n",
        "                self.df[f'{col}_is_outlier'] = self.df.groupby(all_grouping_cols)[col].transform(outlier_func)\n",
        "        else:\n",
        "            print(\"Performing analysis on the entire dataset (no grouping).\")\n",
        "            for col in self.num_cols:\n",
        "                self.df[f'{col}_is_outlier'] = outlier_func(self.df[col])\n",
        "\n",
        "    def _detect_with_decomposition(self):\n",
        "        \"\"\"Handles outlier detection using time series decomposition on each group.\"\"\"\n",
        "        print(f\"Grouping analysis by columns: {self.group_by_cols}\")\n",
        "\n",
        "        # Convert index column to datetime\n",
        "        self.df[self.ts_index_col] = pd.to_datetime(self.df[self.ts_index_col])\n",
        "\n",
        "        for col in self.num_cols:\n",
        "            print(f\"  Analyzing numerical column: '{col}'...\")\n",
        "            # Initialize outlier column with False\n",
        "            self.df[f'{col}_is_outlier'] = False\n",
        "\n",
        "            # Group data and iterate through each group to apply decomposition\n",
        "            grouped = self.df.groupby(self.group_by_cols)\n",
        "            for _, group in grouped:\n",
        "                # Minimum data points needed for decomposition is 2 * period\n",
        "                if len(group) < self.ts_period * 2:\n",
        "                    continue # Skip groups that are too small\n",
        "\n",
        "                # Prepare series for decomposition\n",
        "                series = group.set_index(self.ts_index_col)[col].sort_index()\n",
        "\n",
        "                try:\n",
        "                    # Perform decomposition\n",
        "                    decomposition = seasonal_decompose(\n",
        "                        series, model=self.ts_model, period=self.ts_period, extrapolate_trend='freq'\n",
        "                    )\n",
        "                    # Find outliers in the residual component using the IQR method\n",
        "                    is_outlier_in_residuals = self._find_outliers_iqr(decomposition.resid.dropna())\n",
        "\n",
        "                    # Get the original indices of the outliers and update the main dataframe\n",
        "                    outlier_indices = is_outlier_in_residuals[is_outlier_in_residuals].index\n",
        "                    self.df.loc[self.df.index.isin(group.index) & self.df[self.ts_index_col].isin(outlier_indices), f'{col}_is_outlier'] = True\n",
        "                except Exception as e:\n",
        "                    # Catch potential errors during decomposition (e.g., series not long enough)\n",
        "                    # print(f\"    Could not process group {name} for column '{col}': {e}\")\n",
        "                    pass # Silently continue if a group fails\n",
        "\n",
        "    def _find_outliers_mad(self, series):\n",
        "        \"\"\"Helper function for the Modified Z-score (MAD) method.\"\"\"\n",
        "        median_val = series.median()\n",
        "        abs_deviation = abs(series - median_val)\n",
        "        mad = abs_deviation.median()\n",
        "        if mad == 0:\n",
        "            return pd.Series(False, index=series.index)\n",
        "        modified_z_score = 0.6745 * abs_deviation / mad\n",
        "        return modified_z_score > self.mod_z_score_threshold\n",
        "\n",
        "    def _find_outliers_iqr(self, series):\n",
        "        \"\"\"Helper function for the classic IQR method.\"\"\"\n",
        "        q1 = series.quantile(0.25)\n",
        "        q3 = series.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        # Return a series of False if iqr is 0 to avoid detecting everything as an outlier\n",
        "        if iqr == 0:\n",
        "            return pd.Series(False, index=series.index)\n",
        "        lower_bound = q1 - self.iqr_range * iqr\n",
        "        upper_bound = q3 + self.iqr_range * iqr\n",
        "        return (series < lower_bound) | (series > upper_bound)\n",
        "\n",
        "\n",
        "class GroupOutlierReporter:\n",
        "    \"\"\"\n",
        "    A generic class to generate reports for a specific numerical column\n",
        "    from a DataFrame that has been analyzed by OutlierAnalyzer.\n",
        "    \"\"\"\n",
        "    def __init__(self, analyzed_df, group_by_cols, numeric_col='price'):\n",
        "        \"\"\"\n",
        "        Initialize the reporter.\n",
        "\n",
        "        Parameters:\n",
        "        analyzed_df (pd.DataFrame): The DataFrame with the '_is_outlier' column.\n",
        "        group_by_cols (list): The list of columns that define a group.\n",
        "        numeric_col (str): The name of the numerical column to report on (e.g., 'price').\n",
        "        \"\"\"\n",
        "        self.df = analyzed_df\n",
        "        self.group_by_cols = group_by_cols\n",
        "        self.numeric_col = numeric_col\n",
        "        self.outlier_flag_col = f'{numeric_col}_is_outlier'\n",
        "\n",
        "        if self.outlier_flag_col not in analyzed_df.columns:\n",
        "            raise ValueError(f\"Outlier flag column '{self.outlier_flag_col}' not found. Please run OutlierAnalyzer first.\")\n",
        "\n",
        "        self.outlier_rows = self.df[self.df[self.outlier_flag_col] == True]\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Provides a summary of outlier counts for each group.\"\"\"\n",
        "        if self.outlier_rows.empty:\n",
        "            print(\"No outliers found for the specified column.\")\n",
        "            return pd.DataFrame()\n",
        "        summary = self.outlier_rows.groupby(self.group_by_cols).size().reset_index(name='outliers_count')\n",
        "        return summary.sort_values(by='outliers_count', ascending=False)\n",
        "\n",
        "    def get_group_details(self, group_identifier):\n",
        "        \"\"\"Retrieves the specific outlier rows for a single group.\"\"\"\n",
        "        if not isinstance(group_identifier, tuple) or len(group_identifier) != len(self.group_by_cols):\n",
        "            raise ValueError(f\"group_identifier must be a tuple with {len(self.group_by_cols)} elements.\")\n",
        "\n",
        "        group_data = self.df.copy()\n",
        "        for i, col in enumerate(self.group_by_cols):\n",
        "            group_data = group_data[group_data[col] == group_identifier[i]]\n",
        "\n",
        "        if group_data.empty:\n",
        "            return pd.DataFrame()\n",
        "        return group_data[group_data[self.outlier_flag_col] == True]\n",
        "\n",
        "    def plot_group_boxplot(self, group_identifier, time_col=None):\n",
        "        \"\"\"\n",
        "        Generates a boxplot for a specific group to visualize its distribution.\n",
        "        Can optionally group by a time column within the main group.\n",
        "        \"\"\"\n",
        "        group_data = self.df.copy()\n",
        "        for i, col in enumerate(self.group_by_cols):\n",
        "            group_data = group_data[group_data[col] == group_identifier[i]]\n",
        "\n",
        "        if group_data.empty:\n",
        "            print(f\"Group {group_identifier} not found, cannot generate plot.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Determine plot type based on whether a time_col is provided\n",
        "        x_axis = time_col if time_col else None\n",
        "\n",
        "        sns.boxplot(x=x_axis, y=self.numeric_col, data=group_data)\n",
        "        plt.title(f\"Distribution of '{self.numeric_col}' for: {group_identifier}\")\n",
        "        plt.ylabel(self.numeric_col)\n",
        "        if x_axis:\n",
        "            plt.xlabel(x_axis)\n",
        "            plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNxioZFLpEDX"
      },
      "outputs": [],
      "source": [
        "def visualize_top_outlier_groups(analyzed_df, reporter, summary_df,\n",
        "                                 group_by_cols, numeric_col, top_n=5):\n",
        "    \"\"\"\n",
        "    Generates and displays a consolidated grid of plots for the top N groups\n",
        "    with the most outliers.\n",
        "\n",
        "    This function is designed to be reusable for any dataset analyzed by the\n",
        "    OutlierAnalyzer and GroupOutlierReporter classes.\n",
        "\n",
        "    Parameters:\n",
        "    analyzed_df (pd.DataFrame): The full DataFrame after running OutlierAnalyzer.\n",
        "    reporter (GroupOutlierReporter): The reporter object used for detailed analysis.\n",
        "    summary_df (pd.DataFrame): The summary DataFrame from reporter.get_summary().\n",
        "    group_by_cols (list): The list of columns that define a group.\n",
        "    numeric_col (str): The name of the numerical column being analyzed (e.g., 'price').\n",
        "    top_n (int): The number of top groups to visualize.\n",
        "    \"\"\"\n",
        "    if summary_df.empty:\n",
        "        print(\"No outlier groups to visualize.\")\n",
        "        return\n",
        "\n",
        "    # Determine the grid size for the subplots\n",
        "    top_groups_to_plot = summary_df.head(top_n)\n",
        "    n_plots = len(top_groups_to_plot)\n",
        "    ncols = 3\n",
        "    nrows = (n_plots - 1) // ncols + 1\n",
        "\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 6, nrows * 5))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    print(f\"\\n--- Detailed Visualization for the Top {n_plots} Groups with Outliers ---\")\n",
        "\n",
        "    # Iterate over the top groups and the subplot axes\n",
        "    for i, (index, row) in enumerate(top_groups_to_plot.iterrows()):\n",
        "        # Dynamically create the group identifier from the summary row\n",
        "        target_group = tuple(row[col] for col in group_by_cols)\n",
        "\n",
        "        # Get the full dataset for the current group\n",
        "        full_group_data = analyzed_df.copy()\n",
        "        for col_idx, col_name in enumerate(group_by_cols):\n",
        "            full_group_data = full_group_data[full_group_data[col_name] == target_group[col_idx]]\n",
        "\n",
        "        # Select the current subplot axis\n",
        "        ax = axes[i]\n",
        "\n",
        "        # Define the boolean outlier flag column name dynamically\n",
        "        outlier_flag_col = f\"{numeric_col}_is_outlier\"\n",
        "\n",
        "        # Create a boxplot for the main distribution\n",
        "        sns.boxplot(data=full_group_data, y=numeric_col, ax=ax, color='skyblue', width=0.3, showfliers=False)\n",
        "\n",
        "        # Overlay a stripplot to show individual data points, colored by outlier status\n",
        "        sns.stripplot(data=full_group_data, y=numeric_col, hue=outlier_flag_col,\n",
        "                      palette={True: 'red', False: 'black'}, size=6, ax=ax)\n",
        "\n",
        "        # Customize the subplot for clarity\n",
        "        title_str = '\\n'.join([str(item) for item in target_group])\n",
        "        ax.set_title(f\"Distribution for:\\n{title_str}\", fontsize=10)\n",
        "        ax.set_ylabel(numeric_col)\n",
        "\n",
        "        # Improve legend handling\n",
        "        if ax.get_legend() is not None:\n",
        "            ax.get_legend().set_visible(False)\n",
        "\n",
        "    # Hide any unused subplots in the grid\n",
        "    for i in range(n_plots, len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "\n",
        "    # Adjust layout and show the final plot\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okMjT-TBRMVA"
      },
      "source": [
        "## Cleaning df_food_price_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNChnznxRLRx"
      },
      "outputs": [],
      "source": [
        "duplicated_number = df_food_price_final.duplicated().sum()\n",
        "print(f\"\\n Number of duplicated rows: {duplicated_number}\")\n",
        "\n",
        "# Tally nulls for every column\n",
        "missing_values = df_food_price_final.isnull().sum()\n",
        "print(\"\\n Missing values per column:\")\n",
        "print(missing_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg4BgJipRkEx"
      },
      "outputs": [],
      "source": [
        "# Create a boolean mask to filter for records starting from 2020 onwards.\n",
        "year_mask = df_food_price_final['reference_period_start'] > '2019-12-31 00:00:00'\n",
        "\n",
        "# Apply the mask to the DataFrame to select data from 2020 onwards.\n",
        "# .copy() is used to avoid a SettingWithCopyWarning.\n",
        "df_food_price_selected_0 = df_food_price_final[year_mask].copy()\n",
        "\n",
        "# Apply a custom function to create time-based features (e.g., year, month, quarter).\n",
        "# 'add_progressive_quarter=True' creates a continuous quarter count for time series analysis.\n",
        "df_food_price_selected_1 = time_series_transform(df_food_price_selected_0, column='reference_period_start', add_progressive_quarter=True)\n",
        "\n",
        "# Print the columns to verify the new features have been added.\n",
        "print(df_food_price_selected_1.columns)\n",
        "\n",
        "# Define a list of columns to select and reorder for the final DataFrame.\n",
        "# This ensures the DataFrame has a clean and consistent structure.\n",
        "price_mask = [\n",
        "    'market_name',\n",
        "    'commodity_category',\n",
        "    'commodity_name',\n",
        "    'currency_code',\n",
        "    'price',\n",
        "    'unit',\n",
        "    'reference_period_start',\n",
        "    'reference_period_end',\n",
        "    'admin1_name',\n",
        "    'admin1_code',\n",
        "    'admin2_code',\n",
        "    'geometry',\n",
        "    'lon',\n",
        "    'lat',\n",
        "    'year',\n",
        "    'month',\n",
        "    'quarter',\n",
        "    'progressive_quarter'\n",
        "]\n",
        "\n",
        "# Create the final, clean DataFrame by applying the column mask.\n",
        "df_food_price_selected = df_food_price_selected_1[price_mask]\n",
        "\n",
        "# Display a concise summary of the final DataFrame, including data types and non-null values.\n",
        "print(df_food_price_selected.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_iD4zlEycB9"
      },
      "source": [
        "To identify the outliers and if they are type errors , the price features will be analyze according market_name,commodity_name,unit and year.\n",
        "\n",
        "This because price can have heavy fluctuations across market, commodity, and time, in particular, in crisis areas and developing economies. Three methods of selecting outliers will be tested: the classical IQR method and the more robust median mehtod and the times series decomposition method.\n",
        "\n",
        "Price will be gruped according the market, the type of commodity and its measumerent unit and year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPUv64yeryiS"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "#  ANALYSIS CONFIGURATION & EXECUTION FOR OUTLIERS ANALYSIS\n",
        "# ==============================================================================\n",
        "# Defining the group of categorical features to identfy the oulteliers for all the methdos\n",
        "grouping_food_price = [ 'market_name', 'commodity_name', 'unit']\n",
        "# define time columng for MAD and IQR\n",
        "time_grouping_col='year'\n",
        "# define time column for Time Serie Decomposition\n",
        "ts_index_col='reference_period_start'\n",
        "\n",
        "# --- Run IQR Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING IQR METHOD\\n\" + \"=\"*80)\n",
        "analyzer_iqr = OutlierAnalyzer(df_food_price_selected, time_col=time_grouping_col, group_by_cols=grouping_food_price, method='iqr')\n",
        "df_iqr = analyzer_iqr.detect_outliers()\n",
        "reporter_iqr = GroupOutlierReporter(df_iqr, group_by_cols=grouping_food_price + [time_grouping_col], numeric_col='price')\n",
        "summary_iqr = reporter_iqr.get_summary()\n",
        "print(\"\\n--- Outlier Summary (IQR) ---\")\n",
        "display(summary_iqr)\n",
        "print(\"\\n--- Number of rows and colums (IQR) ---\")\n",
        "print(summary_iqr.info())\n",
        "\n",
        "# --- Run MAD Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING MAD METHOD\\n\" + \"=\"*80)\n",
        "analyzer_mad = OutlierAnalyzer(df_food_price_selected, time_col=time_grouping_col, group_by_cols=grouping_food_price, method='mad')\n",
        "df_mad = analyzer_mad.detect_outliers()\n",
        "reporter_mad = GroupOutlierReporter(df_mad, group_by_cols=grouping_food_price + [time_grouping_col], numeric_col='price')\n",
        "summary_mad = reporter_mad.get_summary()\n",
        "print(\"\\n--- Outlier Summary (MAD) ---\")\n",
        "display(summary_mad)\n",
        "print(\"\\n--- Number of rows and colums (MAD) ---\")\n",
        "print(summary_mad.info())\n",
        "\n",
        "# --- Run Time Series Decomposition Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING TIME SERIES DECOMPOSITION METHOD\\n\" + \"=\"*80)\n",
        "analyzer_ts = OutlierAnalyzer(df_food_price_selected, group_by_cols=grouping_food_price, ts_index_col=ts_index_col, method='ts_decompose', ts_period=12)\n",
        "df_ts = analyzer_ts.detect_outliers()\n",
        "reporter_ts = GroupOutlierReporter(df_ts, group_by_cols=grouping_food_price, numeric_col='price')\n",
        "summary_ts = reporter_ts.get_summary()\n",
        "print(\"\\n--- Outlier Summary (TS Decomposition) ---\")\n",
        "display(summary_ts)\n",
        "print(\"\\n--- Number of rows and colums (TS Decomposition) ---\")\n",
        "print(summary_ts.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYy0GubJvKzg"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. VISUALIZATION AND SAVING PLOTS (CORRECTED)\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- Define the base path for saving images ---\n",
        "file_path = '/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Images'\n",
        "\n",
        "# Create the directory if it doesn't exist to prevent errors\n",
        "os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "# --- Visualize IQR Results ---\n",
        "# Determine the number of plots based on how many groups have outliers\n",
        "n_groups_iqr = len(summary_iqr)\n",
        "fig_iqr = visualize_top_outlier_groups(\n",
        "    analyzed_df=df_iqr,\n",
        "    reporter=reporter_iqr, # <-- ARGOMENTO MANCANTE AGGIUNTO QUI\n",
        "    summary_df=summary_iqr,\n",
        "    group_by_cols=grouping_food_price + [time_grouping_col],\n",
        "    numeric_col='price',\n",
        "    top_n=n_groups_iqr\n",
        ")\n",
        "if fig_iqr:\n",
        "    # --- ⚠️ SOSTITUISCI NOME E PERCORSO DEL FILE SE NECESSARIO ---\n",
        "    file_path_iqr = os.path.join(file_path, \"Price_IQR_Analysis.png\")\n",
        "    fig_iqr.savefig(file_path_iqr, bbox_inches='tight')\n",
        "    print(f\"\\n✅ Grafico analisi IQR salvato in: {file_path_iqr}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Visualize MAD Results ---\n",
        "# Determine the number of plots\n",
        "n_groups_mad = len(summary_mad)\n",
        "fig_mad = visualize_top_outlier_groups(\n",
        "    analyzed_df=df_mad,\n",
        "    reporter=reporter_mad, # <-- ARGOMENTO MANCANTE AGGIUNTO QUI\n",
        "    summary_df=summary_mad,\n",
        "    group_by_cols=grouping_food_price + [time_grouping_col],\n",
        "    numeric_col='price',\n",
        "    top_n=n_groups_mad\n",
        ")\n",
        "if fig_mad:\n",
        "    # --- ⚠️ SOSTITUISCI NOME E PERCORSO DEL FILE SE NECESSARIO ---\n",
        "    file_path_mad = os.path.join(file_path,\"Price_MAD_outlier_analysis.png\")\n",
        "    fig_mad.savefig(file_path_mad, bbox_inches='tight')\n",
        "    print(f\"\\n✅ Grafico analisi MAD salvato in: {file_path_mad}\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- Visualize Time Series Decomposition Results ---\n",
        "# Determine the number of plots\n",
        "n_groups_ts = len(summary_ts)\n",
        "fig_ts = visualize_top_outlier_groups(\n",
        "    analyzed_df=df_ts,\n",
        "    reporter=reporter_ts, # <-- ARGOMENTO MANCANTE AGGIUNTO QUI\n",
        "    summary_df=summary_ts,\n",
        "    group_by_cols=grouping_food_price, # Grouping for TS doesn't include 'year'\n",
        "    numeric_col='price',\n",
        "    top_n=n_groups_ts\n",
        ")\n",
        "if fig_ts:\n",
        "    # --- ⚠️ SOSTITUISCI NOME E PERCORSO DEL FILE SE NECESSARIO ---\n",
        "    file_path_ts = os.path.join(file_path, \"Price_TS_Decomposition_outlier_analysis.png\")\n",
        "    fig_ts.savefig(file_path_ts, bbox_inches='tight')\n",
        "    print(f\"\\n✅ Grafico analisi Time Series salvato in: {file_path_ts}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv-XDU5Fyz3W"
      },
      "outputs": [],
      "source": [
        "# --- Get the summary of outlier groups first ---\n",
        "summary_ts = reporter_ts.get_summary()\n",
        "grouping_cols = reporter_ts.group_by_cols\n",
        "\n",
        "all_outlier_reports = []\n",
        "for index, row in summary_ts.iterrows():\n",
        "  group_identifier = tuple(row[col] for col in grouping_cols)\n",
        "\n",
        "  # --- Get the detailed report for the specific group ---\n",
        "  report = reporter_ts.get_group_details(group_identifier=group_identifier)\n",
        "  # --- Display the report ---\n",
        "  display(report)\n",
        "\n",
        "  # --- Add the report to our list ---\n",
        "  all_outlier_reports.append(report)\n",
        "\n",
        "  # --- After the loop, combine all reports into a single DataFrame ---\n",
        "  final_report_df = pd.concat(all_outlier_reports)\n",
        "\n",
        " # --- Define the path and save the final, consolidated CSV file ---\n",
        "file_path = \"/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Expert Review\"\n",
        "os.makedirs(file_path, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "full_path = os.path.join(file_path, \"food_price_all_outliers_for_review.csv\")\n",
        "\n",
        "# Save to CSV, using index=False to avoid writing the DataFrame index as a column\n",
        "final_report_df.to_csv(full_path, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33yAxmby6BNG"
      },
      "source": [
        "### Key Findings from Outlier Analysis for df_food_price\n",
        "\n",
        "The outlier analysis was conducted using a **Time Series Decomposition** model. This approach is superior to standard statistical methods as it effectively isolates true anomalies (shocks) from underlying data trends (e.g., inflation), providing more contextually relevant insights.\n",
        "\n",
        "Two primary types of anomalies were identified:\n",
        "\n",
        "1.  **Inflationary Shocks**: The analysis successfully distinguished between the general inflationary trend and periods of abnormal price acceleration. Outliers were flagged when the rate of price increase significantly surpassed the established historical trend for a specific commodity-market pair (e.g., Sorghum in Jeremie, H2 2024). These are not just inflation; they are inflationary shocks.\n",
        "\n",
        "2.  **Correlated Market Events**: Strong evidence of market-wide disruptive events was found. Multiple, distinct commodities (e.g., rice and beans in Port-au-Prince) showed simultaneous price anomalies in the same period (e.g., November 2022). This cross-commodity correlation strongly suggests external triggers (such as logistical or security issues) rather than product-specific problems.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "While the model effectively identifies statistically significant anomalies, their definitive validation requires contextual input. The next step is to present these specific findings—particularly the identified \"critical dates\" and \"inflationary shocks\"—to a local food price expert for qualitative assessment and root cause analysis. The generated CSV report of all outliers serves as the primary document for this expert review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyw8YZHM6Adw"
      },
      "outputs": [],
      "source": [
        "### Update df_food_price_final with expert_review ###\n",
        "\n",
        "#--- Load expert review file and dispalay head ---\n",
        "file_path =\"/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Expert Review/food_price_all_outliers_reviewed.csv\"\n",
        "df_expert_reviewed= pd.read_csv(file_path)\n",
        "print(df_expert_reviewed.info())\n",
        "display(df_expert_reviewed.head(5))\n",
        "df_expert_reviewed['reference_period_start'] = pd.to_datetime(df_expert_reviewed['reference_period_start'])\n",
        "df_expert_reviewed['reference_period_end'] = pd.to_datetime(df_expert_reviewed['reference_period_end'])\n",
        "\n",
        "# --- Define the unique key for each row ---\n",
        "# This is crucial for matching rows between the two files.\n",
        "id_cols = ['market_name', 'commodity_name', 'unit', 'reference_period_start']\n",
        "\n",
        "#--- Quantify Changes ---\n",
        "print(\"--- Comparing original report with expert's review ---\")\n",
        "\n",
        "# Use an outer merge to see all differences, additions, and deletions\n",
        "comparison_df = pd.merge(final_report_df,\n",
        "                         df_expert_reviewed,\n",
        "                         on=id_cols,\n",
        "                         how='outer',\n",
        "                         suffixes= (\"_original\",\"_expert\")\n",
        "                         )\n",
        "\n",
        "# --- Calculate the number of changes ---\n",
        "# A row was modified if its price exists in both files but is different.\n",
        "modified_rows= comparison_df[\n",
        "    comparison_df[\"price_original\"].notna() &\n",
        "    comparison_df[\"price_expert\"].notna() &\n",
        "    (comparison_df[\"price_original\"] != comparison_df[\"price_expert\"])\n",
        "]\n",
        "\n",
        "# A row was deleted if it was in the original but is missing from the expert file. This means that expert deleted unchanged rows\n",
        "deleted_rows = comparison_df[comparison_df[\"price_expert\"].isna()]\n",
        "\n",
        "\n",
        "# A row is unchanged if it exists in both and the price is the same.\n",
        "unchanged_rows = comparison_df[\n",
        "    comparison_df[\"price_original\"].notna() &\n",
        "    comparison_df[\"price_expert\"].notna() &\n",
        "    (comparison_df[\"price_original\"] == comparison_df[\"price_expert\"])\n",
        "]\n",
        "\n",
        "# --- Print the summary of changes ---\n",
        "print(\"\\n--- Summary of Expert Changes ---\")\n",
        "print(f\"Rows Modified by Expert: {len(modified_rows)}\")\n",
        "print(f\"Rows Deleted by Expert: {len(deleted_rows)}\")\n",
        "print(f\"Rows Confirmed (Unchanged): {len(unchanged_rows)}\")\n",
        "print(\"=\"*35)\n",
        "\n",
        "# This section updates your main dataset ('df_food_price_selected').\n",
        "print(\"\\n--- Applying changes to the main DataFrame ---\")\n",
        "# The 'expert_reviewed_df' is the source of truth for any row it contains.\n",
        "# We will use the powerful .update() method, which only affects rows\n",
        "# that are present in the expert's file, leaving all others untouched.\n",
        "if not df_expert_reviewed.empty:\n",
        "    # Set index on the main df and the expert df for alignment\n",
        "    df_food_price_selected.set_index(id_cols, inplace=True)\n",
        "    expert_df_for_update = df_expert_reviewed.set_index(id_cols)\n",
        "\n",
        "    # Create a DataFrame containing ONLY the column(s) we want to update.\n",
        "    # In this case, just the 'price'.\n",
        "\n",
        "    updates_to_apply = expert_df_for_update[['price']]\n",
        "    # Update the main df. This modifies values in-place where the indices match.\n",
        "    # Rows not present in 'expert_df_for_update' will NOT be affected.\n",
        "    df_food_price_selected.update(updates_to_apply)\n",
        "\n",
        "    # Reset index to return to original structure\n",
        "    df_food_price_selected.reset_index(inplace=True)\n",
        "    print(f\"  Successfully applied {len(expert_df_for_update)} explicit reviews (modifications/confirmations).\")\n",
        "\n",
        "print(\"\\n✅ Main DataFrame has been successfully updated with expert review.\")\n",
        "print(\"\\n--- Final DataFrame Head ---\")\n",
        "display(df_food_price_selected)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGU-zszNOZe2"
      },
      "source": [
        "## Cleaning df_food_security_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_QGo1U2OZJ4"
      },
      "outputs": [],
      "source": [
        "duplicated_number =df_food_security_final.duplicated().sum()\n",
        "print(f\"\\n Number of duplicated rows: {duplicated_number}\")\n",
        "\n",
        "missing_values = df_food_security_final.isnull().sum()\n",
        "print(\"\\n Missing values per column:\")\n",
        "print(missing_values)\n",
        "df_food_security_final[\"adm1_final\"].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLORATORY DATA ANALYSIS - EDA"
      ],
      "metadata": {
        "id": "ly0cQXybIVUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining EDA Functions"
      ],
      "metadata": {
        "id": "3UB7UpBhIboW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udk1Y_SDsn2d"
      },
      "outputs": [],
      "source": [
        "class UnivariateAnalyzer:\n",
        "    \"\"\"\n",
        "    A class for analyzing and visualizing univariate data, with\n",
        "    special capabilities for time series analysis.\n",
        "    \"\"\"\n",
        "    # --- MODIFICATION 1: __init__ now accepts a time_col for time series analysis ---\n",
        "    def __init__(self, df, time_col=None, columns_to_exclude=None):\n",
        "        \"\"\"\n",
        "        Initialize the UnivariateAnalyzer with a DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame to analyze.\n",
        "        time_col (str, optional): The name of the datetime column to use as the time index.\n",
        "                                  If provided, enables time series analysis features.\n",
        "        columns_to_exclude (list, optional): List of column names to exclude from analysis.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.time_col = time_col\n",
        "        self.columns_to_exclude = columns_to_exclude or []\n",
        "\n",
        "        # --- Time Series Setup ---\n",
        "\n",
        "        # Get all columns by type first, then exclude unwanted ones\n",
        "        all_num_cols = self.df.select_dtypes(include=np.number).columns\n",
        "        all_cat_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "        # Exclude specified columns and the derived year column from general analysis\n",
        "        self.num_cols = [col for col in all_num_cols if col not in self.columns_to_exclude and col != self.time_col]\n",
        "        self.cat_cols = [col for col in all_cat_cols if col not in self.columns_to_exclude]\n",
        "\n",
        "        # Initialize results dictionaries\n",
        "        self.summary_stats = {}\n",
        "        self.categorical_stats = {}\n",
        "        self.yearly_summary = None\n",
        "\n",
        "    # --- MODIFICATION 2: Reworked to calculate yearly or global stats ---\n",
        "    def numerical_summary(self):\n",
        "        \"\"\"\n",
        "        Calculate summary statistics for numerical columns.\n",
        "        If a time_col is set, it calculates the yearly mean for each column.\n",
        "        Otherwise, it calculates global descriptive statistics.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing the summary statistics.\n",
        "        \"\"\"\n",
        "        if self.time_col:\n",
        "            print(f\"--- Calculating Yearly Mean for Numerical Columns (grouped by '{self.time_col}') ---\")\n",
        "            # Group by the year and calculate the mean of all numerical columns\n",
        "            self.time_summary = self.df.groupby(self.time_col)[self.num_cols].mean()\n",
        "            return self.time_summary\n",
        "        else:\n",
        "            print(\"--- Calculating Global Descriptive Statistics (no time grouping) ---\")\n",
        "            self.summary_stats = self.df[self.num_cols].describe()\n",
        "            return self.summary_stats\n",
        "\n",
        "    def categorical_summary(self):\n",
        "        \"\"\"\n",
        "        Calculate summary statistics for categorical columns.\n",
        "        Returns a dictionary of summary DataFrames.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Analyzing Categorical Columns ---\")\n",
        "        for col in self.cat_cols:\n",
        "            # The describe() method is excellent for a quick categorical summary\n",
        "            self.categorical_stats[col] = self.df[col].describe()\n",
        "            print(f\"\\n--- Summary for '{col}' ---\")\n",
        "            display(self.categorical_stats[col].to_frame())\n",
        "\n",
        "\n",
        "    # --- MODIFICATION 3: New, powerful plotting method ---\n",
        "    def plot_analysis(self, figsize=(15, 7)):\n",
        "        \"\"\"\n",
        "        Generate a series of plots for univariate analysis.\n",
        "        If a time_col is set, it prioritizes time series line plots for numerical data.\n",
        "        \"\"\"\n",
        "        # Plot Time Series for numerical columns if a time column is available\n",
        "        if self.time_col:\n",
        "            print(\"\\n\" + \"=\"*75)\n",
        "            print(\"--- Time Series Plots ---\")\n",
        "            print(\"=\"*75)\n",
        "            for col in self.num_cols:\n",
        "                plt.figure(figsize=figsize)\n",
        "                sns.lineplot(data=self.df, x=self.time_col, y=col)\n",
        "                plt.title(f\"Time Series of '{col}'\", fontsize=16)\n",
        "                plt.xlabel(self.time_col)\n",
        "                plt.ylabel(col)\n",
        "                plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "                plt.show()\n",
        "\n",
        "        # Always show distribution of numerical columns\n",
        "        print(\"\\n\" + \"=\"*75)\n",
        "        print(\"--- Distribution of Numerical Columns ---\")\n",
        "        print(\"=\"*75)\n",
        "        for col in self.num_cols:\n",
        "            plt.figure(figsize=figsize)\n",
        "            sns.histplot(self.df[col], kde=True)\n",
        "            plt.title(f\"Histogram of '{col}'\", fontsize=16)\n",
        "            plt.show()\n",
        "\n",
        "        # Always show distribution of categorical columns\n",
        "        print(\"\\n\" + \"=\"*75)\n",
        "        print(\"--- Distribution of Categorical Columns ---\")\n",
        "        print(\"=\"*75)\n",
        "        for col in self.cat_cols:\n",
        "            plt.figure(figsize=figsize)\n",
        "            # Use countplot for better categorical visualization\n",
        "            sns.countplot(y=self.df[col], order = self.df[col].value_counts().index)\n",
        "            plt.title(f\"Value Counts of '{col}'\", fontsize=16)\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUoxOgXkEZWl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data Restructuring: Pivoting Food Security Data from Long to Wide Format\n",
        "The following code block addresses a critical methodological issue in the df_food_security_final DataFrame.\n",
        "The original data is in a 'long' format, where each observation (a specific administrative area at a specific time)\n",
        " is spread across multiple rows, one for each IPC phase (1, 2, 3+, all, etc.).\n",
        "This structure mixes aggregated and disaggregated data, making joins, correlations, and other analyses unreliable.\n",
        "To fix this, we will pivot the DataFrame into a \"wide\" format.\n",
        "\"\"\"\n",
        "\n",
        "# --- START: Transform Food Security data from Long to Wide format ---\n",
        "\n",
        "# Define the columns that uniquely identify each observation (a specific area at a specific time).\n",
        "# These columns will become the index of our new wide DataFrame.\n",
        "grouping_cols = [\n",
        "    'adm1_final',\n",
        "    'adm2_final',\n",
        "    'reference_period_start',\n",
        "    'reference_period_end',\n",
        "    'ipc_type',\n",
        "    'liv_name',\n",
        "    'lz_code1',\n",
        "    'lz_code2',\n",
        "    'lz_code3',\n",
        "    'geometry'\n",
        "]\n",
        "# Perform the pivot operation.\n",
        "# This turns the unique values from the 'ipc_phase' column (like '1', '2', '3+')\n",
        "# into their own separate columns. The cell values will be the 'population_in_phase'.\n",
        "# We fill any missing values with 0, assuming that if a phase is not reported, the population is zero.\n",
        "print(\"Pivoting the df_food_security_final DataFrame...\")\n",
        "# Define the list of value columns we want to pivot into new columns.\n",
        "values_to_pivot = ['population_in_phase', 'population_fraction_in_phase']\n",
        "df_food_security_wide = df_food_security_final.pivot_table(\n",
        "    index=grouping_cols,\n",
        "    columns=['ipc_phase'],\n",
        "    values = values_to_pivot,\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "print(\"Transformation complete. New DataFrame 'df_food_security_final' created.\")\n",
        "display(df_food_security_wide.head())\n",
        "df_food_security_wide.shape\n",
        "df_food_security_wide[\"adm1_final\"].unique()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-YwzKizOr6T"
      },
      "outputs": [],
      "source": [
        "year_mask = df_food_security_final['reference_period_start'] > '2019-12-31 00:00:00'\n",
        "df_food_security_selected_0 = df_food_security_final[year_mask].copy()\n",
        "df_food_security_selected_1= time_series_transform(df_food_security_selected_0, column='reference_period_start', add_progressive_quarter=True)\n",
        "print(df_food_security_selected_1.columns)\n",
        "price_mask=['ipc_phase',\n",
        "            'ipc_type',\n",
        "            'population_in_phase',\n",
        "            'population_fraction_in_phase',\n",
        "            'reference_period_start',\n",
        "            'reference_period_end',\n",
        "            'liv_name',\n",
        "            'adm1_final',\n",
        "            'adm2_final',\n",
        "            'lz_code1',\n",
        "            'lz_code2',\n",
        "            'lz_code3',\n",
        "            'geometry',\n",
        "            'year',\n",
        "            'month',\n",
        "            'quarter',\n",
        "            'progressive_quarter'\n",
        "]\n",
        "df_food_security_selected = df_food_security_selected_1[price_mask]\n",
        "print(df_food_security_selected.info())\n",
        "# To detect outliers and to create a map it is better to split df_food_security_clenaed\n",
        "#in two df, seprarting ipc_type='current' from ipc_type='projection'\n",
        "df_food_security_selected_current= df_food_security_selected[df_food_security_selected['ipc_type']=='current'].copy()\n",
        "df_food_security_selected_projection= df_food_security_selected[df_food_security_selected['ipc_type']=='first projection'].copy() # FIX: Changed 'projection' to 'first projection'\n",
        "print(\"\\n --- Displayin Dataset ---\")\n",
        "print(\"Current Food Security Database\")\n",
        "display(df_food_security_selected_current.head(5))\n",
        "print(\"\\nProjection Food Security Database\")\n",
        "display(df_food_security_selected_projection.head(5))\n",
        "print(df_food_security_final['ipc_phase'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGokZI0ezPCR"
      },
      "outputs": [],
      "source": [
        "# Defining the group of categorical features to identfy the oulteliers for all the methdos\n",
        "grouping_food_price = [ 'market_name', 'commodity_name', 'unit']\n",
        "# define time columng for MAD and IQR\n",
        "time_grouping_col='year'\n",
        "# define time column for Time Serie Decomposition\n",
        "ts_index_col='reference_period_start'\n",
        "\n",
        "# --- Run IQR Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING IQR METHOD\\n\" + \"=\"*80)\n",
        "analyzer_iqr = OutlierAnalyzer(df_food_price_selected, time_col=time_grouping_col, group_by_cols=grouping_food_price, method='iqr')\n",
        "df_iqr = analyzer_iqr.detect_outliers()\n",
        "reporter_iqr = GroupOutlierReporter(df_iqr, group_by_cols=grouping_food_price + [time_grouping_col], numeric_col='price')\n",
        "summary_iqr = reporter_iqr.get_summary()\n",
        "print(\"\\n--- Outlier Summary (IQR) ---\")\n",
        "display(summary_iqr)\n",
        "print(\"\\n--- Number of rows and colums (IQR) ---\")\n",
        "print(summary_iqr.info())\n",
        "\n",
        "# --- Run MAD Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING MAD METHOD\\n\" + \"=\"*80)\n",
        "analyzer_mad = OutlierAnalyzer(df_food_price_selected, time_col=time_grouping_col, group_by_cols=grouping_food_price, method='mad')\n",
        "df_mad = analyzer_mad.detect_outliers()\n",
        "reporter_mad = GroupOutlierReporter(df_mad, group_by_cols=grouping_food_price + [time_grouping_col], numeric_col='price')\n",
        "summary_mad = reporter_mad.get_summary()\n",
        "print(\"\\n--- Outlier Summary (MAD) ---\")\n",
        "display(summary_mad)\n",
        "print(\"\\n--- Number of rows and colums (MAD) ---\")\n",
        "print(summary_mad.info())\n",
        "\n",
        "# --- Run Time Series Decomposition Analysis ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING TIME SERIES DECOMPOSITION METHOD\\n\" + \"=\"*80)\n",
        "analyzer_ts = OutlierAnalyzer(df_food_price_selected, group_by_cols=grouping_food_price, ts_index_col=ts_index_col, method='ts_decompose', ts_period=12)\n",
        "df_ts = analyzer_ts.detect_outliers()\n",
        "reporter_ts = GroupOutlierReporter(df_ts, group_by_cols=grouping_food_price, numeric_col='price')\n",
        "summary_ts = reporter_ts.get_summary()\n",
        "print(\"\\n--- Outlier Summary (TS Decomposition) ---\")\n",
        "display(summary_ts)\n",
        "print(\"\\n--- Number of rows and colums (TS Decomposition) ---\")\n",
        "print(summary_ts.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bae09e7"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# OUTLIERS ANALYSIS FOR PROJECTION FOOD SECURITY DATA\n",
        "# ==============================================================================\n",
        "# We focus on 'population_in_phase' for outlier detection in projection data.\n",
        "\n",
        "# Define the grouping columns for outlier analysis.\n",
        "# We group by the most granular location level available ('liv_name').\n",
        "grouping_food_security_proj = ['liv_name', 'ipc_phase']\n",
        "\n",
        "# Define the time column for Time Series Decomposition.\n",
        "ts_index_col_fs_proj = 'reference_period_start'\n",
        "\n",
        "# --- Run Time Series Decomposition Analysis for Projection Data ---\n",
        "print(\"\\n\" + \"=\"*80 + \"\\nANALYSIS USING TIME SERIES DECOMPOSITION METHOD (PROJECTION FOOD SECURITY)\\n\" + \"=\"*80)\n",
        "analyzer_ts_fs_projection = OutlierAnalyzer(\n",
        "    df_food_security_selected_projection,\n",
        "    group_by_cols=grouping_food_security_proj,\n",
        "    ts_index_col=ts_index_col_fs_proj,\n",
        "    method='ts_decompose',\n",
        "    ts_period=12, # Assuming yearly seasonality\n",
        "    iqr_range=3 # Using a wider range for residuals to be more conservative with outliers\n",
        ")\n",
        "\n",
        "# Detect outliers in 'population_in_phase'\n",
        "df_ts_fs_projection = analyzer_ts_fs_projection.detect_outliers()\n",
        "\n",
        "# Report on outliers in 'population_in_phase'\n",
        "reporter_ts_fs_projection = GroupOutlierReporter(\n",
        "    df_ts_fs_projection,\n",
        "    group_by_cols=grouping_food_security_proj,\n",
        "    numeric_col='population_in_phase' # Report on population\n",
        ")\n",
        "\n",
        "summary_ts_fs_projection = reporter_ts_fs_projection.get_summary()\n",
        "print(\"\\n--- Outlier Summary (TS Decomposition - Projection Food Security) ---\")\n",
        "display(summary_ts_fs_projection)\n",
        "\n",
        "print(\"\\n--- Number of rows and columns (TS Decomposition - Projection Food Security Summary) ---\")\n",
        "print(summary_ts_fs_projection.info())\n",
        "\n",
        "# Optionally, visualize the top outlier groups for projection data\n",
        "if not summary_ts_fs_projection.empty:\n",
        "    print(\"\\n--- Visualizing Top Outlier Groups (Projection Food Security) ---\")\n",
        "    # Determine the number of groups to visualize (e.g., top 5 or fewer if fewer exist)\n",
        "    n_groups_to_plot_proj = min(5, len(summary_ts_fs_projection))\n",
        "\n",
        "    fig_ts_fs_projection = visualize_top_outlier_groups(\n",
        "        analyzed_df=df_ts_fs_projection,\n",
        "        reporter=reporter_ts_fs_projection,\n",
        "        summary_df=summary_ts_fs_projection,\n",
        "        group_by_cols=grouping_food_security_proj,\n",
        "        numeric_col='population_in_phase',\n",
        "        top_n=n_groups_to_plot_proj # Plot only the top N groups\n",
        "    )\n",
        "    if fig_ts_fs_projection:\n",
        "        # Define the base path for saving images and create the directory if it doesn't exist\n",
        "        file_path = '/content/drive/MyDrive/Data Science/PORTFOLIO/HAITI/Images'\n",
        "        os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "        file_path_ts_fs_projection = os.path.join(file_path, \"Food_Security_TS_Decomposition_outlier_analysis_Projection.png\")\n",
        "        fig_ts_fs_projection.savefig(file_path_ts_fs_projection, bbox_inches='tight')\n",
        "        print(f\"\\n✅ Grafico analisi Time Series (Projection FS) salvato in: {file_path_ts_fs_projection}\")\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"\\nNo outlier groups to visualize for Projection Food Security data.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}